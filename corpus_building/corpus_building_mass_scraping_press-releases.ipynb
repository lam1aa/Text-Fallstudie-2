{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e169cfbb-5409-4bc0-82db-413343b58e32",
   "metadata": {},
   "source": [
    "# üöÄ Massenscraping von Pressemitteilungen"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8db05ff-df64-4e47-aea4-beb1008c13d4",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## Hinweise zur Ausf√ºhrung des Notebooks\n",
    "Dieses Notebook kann auf unterschiedlichen Levels erarbeitet werden (siehe Abschnitt [\"Technische Voraussetzungen\"](../markdown/introduction_requirements)): \n",
    "1. Book-Only Mode\n",
    "2. Cloud Mode: Daf√ºr auf üöÄ klicken und z.B. in Colab ausf√ºhren.\n",
    "3. Local Mode: Daf√ºr auf Herunterladen ‚Üì klicken und \".ipynb\" w√§hlen. \n",
    "\n",
    "## √úbersicht\n",
    "\n",
    "Im Folgenden werden alle Pressemitteilungen der Berliner Staatskanzlei gescraped\n",
    "\n",
    "Daf√ºr werden folgendene Schritte durchgef√ºhrt:\n",
    "1. Wir werden die Struktur des Teils der Website untersuchen, der alle Pressemitteilungen enth√§lt.\n",
    "2. Wir werden die URL-Links zu allen Pressemitteilungen abrufen.\n",
    "3. Abschlie√üend werden wir alle Pressemitteilungen scrapen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0fb10a4-c384-4b7e-a956-2fa13cb1a34c",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "hide-cell"
    ]
   },
   "outputs": [],
   "source": [
    "# üöÄ Install libraries \n",
    "!pip install requests tqdm "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4b5c5d7a-f727-4385-8c30-49347b73d692",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "hide-cell"
    ]
   },
   "outputs": [],
   "source": [
    "import requests, pathlib, time, re, logging, textwrap\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a371973-de38-4439-8878-86b31954abea",
   "metadata": {},
   "source": [
    "## Abruf und Analyse der Suchseite f√ºr Pressemitteilungen"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b470b17-54f4-4ae5-8d97-581780c8b86b",
   "metadata": {},
   "source": [
    "Im Kapitel ['Aufbau des Forschungskorpus'](../corpus_collection/corpus-collection_building-our-corpus.html#aufbau-des-forschungskorpus) haben wir die Auswahl- und Filterprozesse f√ºr unser Korpus von Pressemitteilungen beschrieben. Nun geht es darum, das Korpus mithilfe von Scraping-Tools und HTML-Kenntnissen zu extrahieren. <!-- In the chapter ['Aufbau des Forschungskorpus'](../corpus_collection/corpus-collection_building-our-corpus.html#aufbau-des-forschungskorpus) we have outlined the selection & filtering process for our corpus of press releases. Now it is time to imlement scraping of tha corpus using scraping tools and knowledge of HTML. -->\n",
    "\n",
    "1. Wir wissen bereits, dass das [Suchmen√º](https://www.berlin.de/presse/pressemitteilungen/index/search) auf der Website Berlin.de gezielt die Auswahl der f√ºr uns interessanten Abteilungen erm√∂glicht. <!-- We already know that the [Search menu](https://www.berlin.de/presse/pressemitteilungen/index/search) on the Berlin.de website allows to select only the departments that interest us: -->\n",
    "   \n",
    "![selection](../book_images/selection_of_depts.png)\n",
    "\n",
    "2. Anschlie√üend k√∂nnen wir mit den [ausgew√§hlten Abteilungen und einer leeren Suchanfrage suchen](https://www.berlin.de/presse/pressemitteilungen/index/search/?searchtext=&boolean=0&startdate=&enddate=&alle-senatsverwaltungen=on&institutions%5B%5D=Presse-+und+Informationsamt+des+Landes+Berlin&institutions%5B%5D=Senatsverwaltung+f√ºr+Bildung%2C+Jugend+und+Familie&institutions%5B%5D=Senatsverwaltung+f√ºr+Finanzen&institutions%5B%5D=Senatsverwaltung+f√ºr+Inneres+und+Sport&institutions%5B%5D=Senatsverwaltung+f√ºr+Arbeit%2C+Soziales%2C+Gleichstellung%2C+Integration%2C+Vielfalt+und+Antidiskriminierung&institutions%5B%5D=Senatsverwaltung+f√ºr+Justiz+und+Verbraucherschutz&institutions%5B%5D=Senatsverwaltung+f√ºr+Kultur+und+Gesellschaftlichen+Zusammenhalt&institutions%5B%5D=Senatsverwaltung+f√ºr+Stadtentwicklung%2C+Bauen+und+Wohnen&institutions%5B%5D=Senatsverwaltung+f√ºr+Mobilit√§t%2C+Verkehr%2C+Klimaschutz+und+Umwelt&institutions%5B%5D=Senatsverwaltung+f√ºr+Wirtschaft%2C+Energie+und+Betriebe&institutions%5B%5D=Senatsverwaltung+f√ºr+Wissenschaft%2C+Gesundheit+und+Pflege&alle-bezirksamt=on&institutions%5B%5D=Bezirksamt+Charlottenburg-Wilmersdorf&institutions%5B%5D=Bezirksamt+Friedrichshain-Kreuzberg&institutions%5B%5D=Bezirksamt+Lichtenberg&institutions%5B%5D=Bezirksamt+Marzahn-Hellersdorf&institutions%5B%5D=Bezirksamt+Mitte&institutions%5B%5D=Bezirksamt+Neuk√∂lln&institutions%5B%5D=Bezirksamt+Pankow&institutions%5B%5D=Bezirksamt+Reinickendorf&institutions%5B%5D=Bezirksamt+Spandau&institutions%5B%5D=Bezirksamt+Steglitz-Zehlendorf&institutions%5B%5D=Bezirksamt+Tempelhof-Sch√∂neberg&institutions%5B%5D=Bezirksamt+Treptow-K√∂penick&alle-landesbeauftragte=on&institutions%5B%5D=Beauftragte+des+Senats+f√ºr+Integration+und+Migration&institutions%5B%5D=Beauftragter+zur+Aufarbeitung+der+SED-Diktatur&institutions%5B%5D=B√ºrger-+und+Polizeibeauftragter+des+Landes+Berlin&institutions%5B%5D=Pflegebeauftragte+des+Landes+Berlin&institutions%5B%5D=Landestierschutzbeauftragte&institutions%5B%5D=Landeswahlleitung&bt=) und so alle Pressemitteilungen dieser Abteilungen abrufen: <!-- After that we can [perform search with selected depatrmentes and an empty query](https://www.berlin.de/presse/pressemitteilungen/index/search/?searchtext=&boolean=0&startdate=&enddate=&alle-senatsverwaltungen=on&institutions%5B%5D=Presse-+und+Informationsamt+des+Landes+Berlin&institutions%5B%5D=Senatsverwaltung+f√ºr+Bildung%2C+Jugend+und+Familie&institutions%5B%5D=Senatsverwaltung+f√ºr+Finanzen&institutions%5B%5D=Senatsverwaltung+f√ºr+Inneres+und+Sport&institutions%5B%5D=Senatsverwaltung+f√ºr+Arbeit%2C+Soziales%2C+Gleichstellung%2C+Integration%2C+Vielfalt+und+Antidiskriminierung&institutions%5B%5D=Senatsverwaltung+f√ºr+Justiz+und+Verbraucherschutz&institutions%5B%5D=Senatsverwaltung+f√ºr+Kultur+und+Gesellschaftlichen+Zusammenhalt&institutions%5B%5D=Senatsverwaltung+f√ºr+Stadtentwicklung%2C+Bauen+und+Wohnen&institutions%5B%5D=Senatsverwaltung+f√ºr+Mobilit√§t%2C+Verkehr%2C+Klimaschutz+und+Umwelt&institutions%5B%5D=Senatsverwaltung+f√ºr+Wirtschaft%2C+Energie+und+Betriebe&institutions%5B%5D=Senatsverwaltung+f√ºr+Wissenschaft%2C+Gesundheit+und+Pflege&alle-bezirksamt=on&institutions%5B%5D=Bezirksamt+Charlottenburg-Wilmersdorf&institutions%5B%5D=Bezirksamt+Friedrichshain-Kreuzberg&institutions%5B%5D=Bezirksamt+Lichtenberg&institutions%5B%5D=Bezirksamt+Marzahn-Hellersdorf&institutions%5B%5D=Bezirksamt+Mitte&institutions%5B%5D=Bezirksamt+Neuk√∂lln&institutions%5B%5D=Bezirksamt+Pankow&institutions%5B%5D=Bezirksamt+Reinickendorf&institutions%5B%5D=Bezirksamt+Spandau&institutions%5B%5D=Bezirksamt+Steglitz-Zehlendorf&institutions%5B%5D=Bezirksamt+Tempelhof-Sch√∂neberg&institutions%5B%5D=Bezirksamt+Treptow-K√∂penick&alle-landesbeauftragte=on&institutions%5B%5D=Beauftragte+des+Senats+f√ºr+Integration+und+Migration&institutions%5B%5D=Beauftragter+zur+Aufarbeitung+der+SED-Diktatur&institutions%5B%5D=B√ºrger-+und+Polizeibeauftragter+des+Landes+Berlin&institutions%5B%5D=Pflegebeauftragte+des+Landes+Berlin&institutions%5B%5D=Landestierschutzbeauftragte&institutions%5B%5D=Landeswahlleitung&bt=) and retrieve all press releases belonging to these departments:-->\n",
    "\n",
    "![suchergebnisse](../book_images/suchergebnisse.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd435155-4cbf-456d-b39e-955b99b362e8",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "Wir sehen, dass die Links hier in einer Tabelle gespeichert sind. In HTML wird eine Tabelle mit dem `<table>`-Element dargestellt. Wenn wir den Quellcode dieser Seite betrachten, stellen wir fest, dass sie eine Tabelle enth√§lt, in der alle Links aufgef√ºhrt sind:\n",
    "\n",
    "![selection](../book_images/pm_table_source_html.png) \n",
    "\n",
    "Um diese Links zu durchsuchen, k√∂nnen wir die grundlegenden HTML-Abfragefunktionen der bereits bekannten Bibliothek BeautifulSoup verwenden. Das machen wir im n√§chsten Abschnitt."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd62da74-4319-486d-adee-9058dbc9f495",
   "metadata": {},
   "source": [
    "## Suchergebnisse scrapen und Pressemitteilungen extrahieren (auf einer Seite): "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f973641b-67cf-4a7c-8f28-a51b92e03e07",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "skip-execution"
    ]
   },
   "outputs": [],
   "source": [
    "# -- organise data ----------------------------------------------------\n",
    "SAMPLE_OUTPUT_PAGE = (\n",
    "    \"https://www.berlin.de/presse/pressemitteilungen/index/search/?searchtext=&boolean=0&startdate=&enddate=&alle-senatsverwaltungen=on&institutions%5B%5D=Presse-+und+Informationsamt+des+Landes+Berlin&institutions%5B%5D=Senatsverwaltung+f√ºr+Bildung%2C+Jugend+und+Familie&institutions%5B%5D=Senatsverwaltung+f√ºr+Finanzen&institutions%5B%5D=Senatsverwaltung+f√ºr+Inneres+und+Sport&institutions%5B%5D=Senatsverwaltung+f√ºr+Arbeit%2C+Soziales%2C+Gleichstellung%2C+Integration%2C+Vielfalt+und+Antidiskriminierung&institutions%5B%5D=Senatsverwaltung+f√ºr+Justiz+und+Verbraucherschutz&institutions%5B%5D=Senatsverwaltung+f√ºr+Kultur+und+Gesellschaftlichen+Zusammenhalt&institutions%5B%5D=Senatsverwaltung+f√ºr+Stadtentwicklung%2C+Bauen+und+Wohnen&institutions%5B%5D=Senatsverwaltung+f√ºr+Mobilit√§t%2C+Verkehr%2C+Klimaschutz+und+Umwelt&institutions%5B%5D=Senatsverwaltung+f√ºr+Wirtschaft%2C+Energie+und+Betriebe&institutions%5B%5D=Senatsverwaltung+f√ºr+Wissenschaft%2C+Gesundheit+und+Pflege&alle-bezirksamt=on&institutions%5B%5D=Bezirksamt+Charlottenburg-Wilmersdorf&institutions%5B%5D=Bezirksamt+Friedrichshain-Kreuzberg&institutions%5B%5D=Bezirksamt+Lichtenberg&institutions%5B%5D=Bezirksamt+Marzahn-Hellersdorf&institutions%5B%5D=Bezirksamt+Mitte&institutions%5B%5D=Bezirksamt+Neuk√∂lln&institutions%5B%5D=Bezirksamt+Pankow&institutions%5B%5D=Bezirksamt+Reinickendorf&institutions%5B%5D=Bezirksamt+Spandau&institutions%5B%5D=Bezirksamt+Steglitz-Zehlendorf&institutions%5B%5D=Bezirksamt+Tempelhof-Sch√∂neberg&institutions%5B%5D=Bezirksamt+Treptow-K√∂penick&alle-landesbeauftragte=on&institutions%5B%5D=Beauftragte+des+Senats+f√ºr+Integration+und+Migration&institutions%5B%5D=Beauftragter+zur+Aufarbeitung+der+SED-Diktatur&institutions%5B%5D=B√ºrger-+und+Polizeibeauftragter+des+Landes+Berlin&institutions%5B%5D=Pflegebeauftragte+des+Landes+Berlin&institutions%5B%5D=Landestierschutzbeauftragte&institutions%5B%5D=Landeswahlleitung&bt=\"\n",
    ") \n",
    "DATA_DIR   = pathlib.Path(\"../data-updated\")          # keep same root as repo\n",
    "HTML_DIR   = DATA_DIR / \"html\"\n",
    "TXT_DIR    = DATA_DIR / \"txt\"\n",
    "HTML_DIR.mkdir(parents=True, exist_ok=True)\n",
    "TXT_DIR.mkdir(parents=True,  exist_ok=True)\n",
    "logging.basicConfig(level=logging.INFO, format=\"%(levelname)s: %(message)s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6d83d1d3-86af-41db-a8db-b14b270d17bb",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "skip-execution"
    ]
   },
   "outputs": [],
   "source": [
    "# -- helper -----------------------------------------------------------------\n",
    "\n",
    "def get_soup(url: str) -> BeautifulSoup:\n",
    "    \"\"\"Download a page and return BeautifulSoup (retry politely on transient errors).\"\"\"\n",
    "    while True:\n",
    "        r = requests.get(\n",
    "            url,\n",
    "            timeout=20,\n",
    "            headers={\"User-Agent\": \"Mozilla/5.0 (compatible; QuadrigaScraper/1.0)\"}\n",
    "        )\n",
    "        if r.status_code == 200:\n",
    "            return BeautifulSoup(r.text, \"lxml\")\n",
    "        logging.warning(\"Status %s on %s ‚Äì retrying in 5 s\", r.status_code, url)\n",
    "        time.sleep(5)\n",
    "\n",
    "def slugify(text_: str, maxlen: int = 60) -> str:\n",
    "    \"\"\"Rough filename-safe slug for headlines.\"\"\"\n",
    "    text_ = re.sub(r\"\\W+\", \"-\", text_.lower()).strip(\"-\")\n",
    "    return text_[:maxlen] or \"untitled\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a49834fb-f7c2-4811-8319-369c5cbc6bb9",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "<button onclick=\"myFunction()\">Was passiert hier oben? (Schritt-f√ºr-Schritt-Erkl√§rung)</button>\n",
    "\n",
    "<div id=\"myDIV\" style=\"display:none\">\n",
    "    \n",
    "### Was passiert in diesem Codeblock oben?\n",
    "\n",
    "1. **`get_soup()`**\n",
    "\n",
    "   * Die Funktion versucht, eine Webseite herunterzuladen und sofort als `BeautifulSoup`-Objekt zur√ºckzugeben.\n",
    "   * Sie sendet einen HTTP-Request mit\n",
    "\n",
    "     * 20 Sekunden Timeout (sch√ºtzt vor ewig h√§ngenden Verbindungen) und\n",
    "     * einem eigenen *User-Agent*-Header, damit der Server wei√ü, dass es sich um ein automatisiertes, aber h√∂fliches Skript handelt (‚ÄûQuadrigaScraper/1.0‚Äú).\n",
    "   * Wenn der Server **Status 200** liefert ‚Üí der HTML-Text wird geparst und zur√ºckgegeben.\n",
    "   * Bei jedem anderen Statuscode wird eine Warnung ins Log geschrieben, 5 Sekunden gewartet und dann erneut versucht. Dadurch bricht der Scraper nicht sofort ab, sondern behandelt tempor√§re Fehler (z. B. 500er oder Netzwerk-Glitches) selbstst√§ndig.\n",
    "\n",
    "2. **`slugify()`**\n",
    "\n",
    "   * Wandelt eine √úberschrift (oder beliebigen Text) in einen dateisystemtauglichen ‚ÄûSlug‚Äú um.\n",
    "   * Schritte:\n",
    "\n",
    "     1. alles in Kleinbuchstaben verwandeln,\n",
    "     2. alle Zeichen, die **nicht** Buchstaben, Ziffern oder Unterstrich sind, durch Bindestriche ersetzen (`\\W+`),\n",
    "     3. f√ºhrende/abschlie√üende Bindestriche entfernen,\n",
    "     4. das Ergebnis auf maximal 60 Zeichen k√ºrzen (damit Dateinamen handlich bleiben).\n",
    "   * Falls nach allen Filterungen nichts √ºbrig ist, liefert die Funktion sicherheitshalber den Platzhalter **‚Äûuntitled‚Äú** zur√ºck.\n",
    "\n",
    "</div>\n",
    "\n",
    "<script>\n",
    "function myFunction() {\n",
    "  var x = document.getElementById(\"myDIV\");\n",
    "  if (x.style.display === \"none\") {\n",
    "    x.style.display = \"block\";\n",
    "  } else {\n",
    "    x.style.display = \"none\";\n",
    "  }\n",
    "}\n",
    "</script>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "577fdf92-4f28-4a8a-8dfa-5c607098d6a9",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "skip-execution"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 10 rows on the page.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Rows: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 [00:05<00:00,  1.73it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>ressort</th>\n",
       "      <th>title</th>\n",
       "      <th>pr_url</th>\n",
       "      <th>filename_html</th>\n",
       "      <th>filename_txt</th>\n",
       "      <th>n_tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>16.06.2025</td>\n",
       "      <td>Senatsverwaltung f√ºr Wirtschaft, Energie und B...</td>\n",
       "      <td>Berlin stellt neues Esport-Nachwuchsteam vor ‚Äì...</td>\n",
       "      <td>https://www.berlin.de/sen/web/presse/pressemit...</td>\n",
       "      <td>1570469.html</td>\n",
       "      <td>1570469.txt</td>\n",
       "      <td>933</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>16.06.2025</td>\n",
       "      <td>Bezirksamt Spandau</td>\n",
       "      <td>Drei Sprachen, ein Tag: Dein Tor zum Norden</td>\n",
       "      <td>https://www.berlin.de/ba-spandau/aktuelles/pre...</td>\n",
       "      <td>1570459.html</td>\n",
       "      <td>1570459.txt</td>\n",
       "      <td>754</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>16.06.2025</td>\n",
       "      <td>Bezirksamt Spandau</td>\n",
       "      <td>Schnupperkurs Qi Gong in der Stadtbibliothek S...</td>\n",
       "      <td>https://www.berlin.de/ba-spandau/aktuelles/pre...</td>\n",
       "      <td>1570455.html</td>\n",
       "      <td>1570455.txt</td>\n",
       "      <td>658</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>16.06.2025</td>\n",
       "      <td>Bezirksamt Spandau</td>\n",
       "      <td>Digitaltag 2025: Digitalisierung von Archivbes...</td>\n",
       "      <td>https://www.berlin.de/ba-spandau/aktuelles/pre...</td>\n",
       "      <td>1570447.html</td>\n",
       "      <td>1570447.txt</td>\n",
       "      <td>704</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>16.06.2025</td>\n",
       "      <td>Bezirksamt Treptow-K√∂penick</td>\n",
       "      <td>Kunst am Bau: Entscheidung im Kunstwettbewerb ...</td>\n",
       "      <td>https://www.berlin.de/ba-treptow-koepenick/akt...</td>\n",
       "      <td>1570440.html</td>\n",
       "      <td>1570440.txt</td>\n",
       "      <td>1008</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         date                                            ressort  \\\n",
       "0  16.06.2025  Senatsverwaltung f√ºr Wirtschaft, Energie und B...   \n",
       "1  16.06.2025                                 Bezirksamt Spandau   \n",
       "2  16.06.2025                                 Bezirksamt Spandau   \n",
       "3  16.06.2025                                 Bezirksamt Spandau   \n",
       "4  16.06.2025                        Bezirksamt Treptow-K√∂penick   \n",
       "\n",
       "                                               title  \\\n",
       "0  Berlin stellt neues Esport-Nachwuchsteam vor ‚Äì...   \n",
       "1        Drei Sprachen, ein Tag: Dein Tor zum Norden   \n",
       "2  Schnupperkurs Qi Gong in der Stadtbibliothek S...   \n",
       "3  Digitaltag 2025: Digitalisierung von Archivbes...   \n",
       "4  Kunst am Bau: Entscheidung im Kunstwettbewerb ...   \n",
       "\n",
       "                                              pr_url filename_html  \\\n",
       "0  https://www.berlin.de/sen/web/presse/pressemit...  1570469.html   \n",
       "1  https://www.berlin.de/ba-spandau/aktuelles/pre...  1570459.html   \n",
       "2  https://www.berlin.de/ba-spandau/aktuelles/pre...  1570455.html   \n",
       "3  https://www.berlin.de/ba-spandau/aktuelles/pre...  1570447.html   \n",
       "4  https://www.berlin.de/ba-treptow-koepenick/akt...  1570440.html   \n",
       "\n",
       "  filename_txt  n_tokens  \n",
       "0  1570469.txt       933  \n",
       "1  1570459.txt       754  \n",
       "2  1570455.txt       658  \n",
       "3  1570447.txt       704  \n",
       "4  1570440.txt      1008  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# -- step 1: parse ONE results page -----------------------------------------\n",
    "search_soup = get_soup(SAMPLE_OUTPUT_PAGE)\n",
    "\n",
    "rows = search_soup.select(\"table tbody tr\")  \n",
    "records = []\n",
    "print(f\"Found {len(rows)} rows on the page.\")\n",
    "\n",
    "for tr in tqdm(rows, desc=\"Rows\"):\n",
    "    # grab all data cells once\n",
    "    cells = tr.find_all(\"td\")\n",
    "    if len(cells) < 3:          # footer / empty rows ‚Üí ignore\n",
    "        continue\n",
    "\n",
    "    # column 1 ‚Äì date\n",
    "    date_txt = cells[0].get_text(strip=True)\n",
    "\n",
    "    # column 2 ‚Äì headline + link\n",
    "    anchor = cells[1].find(\"a\", href=True)\n",
    "    if anchor is None:          # safety check\n",
    "        continue\n",
    "    title   = anchor.get_text(strip=True)\n",
    "    pr_url  = \"https://www.berlin.de\" + anchor[\"href\"]\n",
    "\n",
    "    # column 3 ‚Äì issuing authority (‚ÄúRessort‚Äù)\n",
    "    ressort = cells[2].get_text(strip=True)\n",
    "\n",
    "    # deterministic ID, e.g. 1570469\n",
    "    uid = anchor[\"href\"].split(\"/\")[-1].split(\".\")[-2]\n",
    "\n",
    "        # -- step 2: download the press release itself -----------\n",
    "    html_file = HTML_DIR / f\"{uid}.html\"\n",
    "    txt_file  = TXT_DIR  / f\"{uid}.txt\"\n",
    "\n",
    "    if not html_file.exists():        # skip if already scraped\n",
    "        pr_soup = get_soup(pr_url)\n",
    "\n",
    "        # write raw HTML\n",
    "        html_file.write_text(str(pr_soup), encoding=\"utf-8\")\n",
    "\n",
    "        # extract main text; fallback to whole page if CSS id changes\n",
    "        body = (pr_soup.select_one(\"#article\") or        # new layout (2024)\n",
    "                pr_soup.select_one(\"#content\") or        # classic layout\n",
    "                pr_soup)                                 # last resort\n",
    "        clean_text = body.get_text(\" \", strip=True)\n",
    "        txt_file.write_text(clean_text, encoding=\"utf-8\")\n",
    "    else:\n",
    "        # we still need the plain text length for the DataFrame below\n",
    "        clean_text = txt_file.read_text(encoding=\"utf-8\")\n",
    "\n",
    "    records.append(\n",
    "        dict(\n",
    "            date=date_txt,\n",
    "            ressort=ressort,\n",
    "            title=title,\n",
    "            pr_url=pr_url,\n",
    "            filename_html=html_file.name,\n",
    "            filename_txt=txt_file.name,\n",
    "            n_tokens=len(clean_text.split())\n",
    "        )\n",
    "    )\n",
    "    time.sleep(0.4)      # politeness\n",
    "\n",
    "# -- step 3: inspect the harvested metadata ---------------------------------\n",
    "df = pd.DataFrame(records)\n",
    "df.head()          # normal Jupyter display is fine ‚Äï no extra libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cba200c-4ff3-4606-8ad1-06814b431af2",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "<button onclick=\"myFunction()\">Was passiert hier oben? (Schritt-f√ºr-Schritt-Erkl√§rung)</button>\n",
    "\n",
    "<div id=\"myDIV2\" style=\"display:none\">\n",
    "\n",
    "### Was passiert in diesem Codeblock oben? ‚Äì Schritt f√ºr Schritt\n",
    "\n",
    "1. **Erste Seite einlesen**\n",
    "\n",
    "   ```python\n",
    "   search_soup = get_soup(SAMPLE_OUTPUT_PAGE)\n",
    "   rows = search_soup.select(\"table tbody tr\")\n",
    "   ```\n",
    "\n",
    "   * Die Funktion `get_soup()` l√§dt genau **eine** Ergebnisseite der\n",
    "     Such-/Listenansicht und gibt sie als Beautiful-Soup-Objekt zur√ºck.\n",
    "   * Mit dem CSS-Selektor `table tbody tr` werden alle Tabellenzeilen\n",
    "     der Ergebnisliste eingesammelt. Jede Zeile repr√§sentiert eine\n",
    "     einzelne Pressemitteilung.\n",
    "\n",
    "2. **Vorbereitung f√ºr die Schleife**\n",
    "\n",
    "   ```python\n",
    "   records = []\n",
    "   print(f\"Found {len(rows)} rows on the page.\")\n",
    "   ```\n",
    "\n",
    "   * `records` soll sp√§ter eine Liste von Dictionaries f√ºr das\n",
    "     DataFrame sammeln.\n",
    "   * Eine kurze Ausgabe zeigt, wie viele Zeilen tats√§chlich gefunden\n",
    "     wurden ‚Äì n√ºtzlich f√ºr Kontroll-/Debug-Zwecke.\n",
    "\n",
    "3. **Iterieren mit Fortschrittsbalken**\n",
    "\n",
    "   ```python\n",
    "   for tr in tqdm(rows, desc=\"Rows\"):\n",
    "   ```\n",
    "\n",
    "   * `tqdm` liefert einen h√ºbschen Fortschrittsbalken ‚Äì perfekt f√ºr\n",
    "     Lehr- und Live-Demos.\n",
    "\n",
    "4. **Zellen extrahieren & Plausibilit√§t pr√ºfen**\n",
    "\n",
    "   ```python\n",
    "   cells = tr.find_all(\"td\")\n",
    "   if len(cells) < 3:          # footer / empty rows ‚Üí ignore\n",
    "       continue\n",
    "   ```\n",
    "\n",
    "   * Alle `<td>` einer Zeile werden auf einmal geholt.\n",
    "   * Hat eine Zeile weniger als drei Zellen, handelt es sich um\n",
    "     Paginierungs- oder Leerzeilen; die werden √ºbersprungen.\n",
    "\n",
    "5. **Spalte 1 ‚Äì Datum**\n",
    "\n",
    "   ```python\n",
    "   date_txt = cells[0].get_text(strip=True)\n",
    "   ```\n",
    "\n",
    "   * `strip=True` entfernt Zeilenumbr√ºche und Leerzeichen ‚Äì wir erhalten\n",
    "     saubere Strings wie ‚Äû16.06.2025‚Äú.\n",
    "\n",
    "6. **Spalte 2 ‚Äì √úberschrift & Link**\n",
    "\n",
    "   ```python\n",
    "   anchor = cells[1].find(\"a\", href=True)\n",
    "   if anchor is None:\n",
    "       continue\n",
    "   title  = anchor.get_text(strip=True)\n",
    "   pr_url = \"https://www.berlin.de\" + anchor[\"href\"]\n",
    "   ```\n",
    "\n",
    "   * Innerhalb der zweiten Zelle steckt der anklickbare Link.\n",
    "   * Sicherheits-Check: Falls doch kein `<a>` vorhanden ist, Zeile\n",
    "     √ºberspringen.\n",
    "   * Die relative URL wird zur vollst√§ndigen URL erg√§nzt.\n",
    "\n",
    "7. **Spalte 3 ‚Äì Ressort (herausgebende Beh√∂rde)**\n",
    "\n",
    "   ```python\n",
    "   ressort = cells[2].get_text(strip=True)\n",
    "   ```\n",
    "\n",
    "8. **Eindeutige ID ableiten**\n",
    "\n",
    "   ```python\n",
    "   uid = anchor[\"href\"].split(\"/\")[-1].split(\".\")[-2]\n",
    "   ```\n",
    "\n",
    "   * Vom Pfadsegment `pressemitteilung.1570469.php` wird mittels\n",
    "     `split()` das numerische St√ºck **1570469** herausgel√∂st.\n",
    "   * Diese ID landet sp√§ter im Dateinamen, damit jeder Release genau\n",
    "     eine HTML- und eine TXT-Datei bekommt.\n",
    "\n",
    "9. **Dateipfade festlegen**\n",
    "\n",
    "   ```python\n",
    "   html_file = HTML_DIR / f\"{uid}.html\"\n",
    "   txt_file  = TXT_DIR  / f\"{uid}.txt\"\n",
    "   ```\n",
    "\n",
    "10. **HTML herunterladen & Text extrahieren (nur falls neu)**\n",
    "\n",
    "    ```python\n",
    "    if not html_file.exists():\n",
    "        pr_soup = get_soup(pr_url)\n",
    "        html_file.write_text(str(pr_soup), encoding=\"utf-8\")\n",
    "\n",
    "        body = (pr_soup.select_one(\"#article\")     # neues Layout\n",
    "                or pr_soup.select_one(\"#content\")  # altes Layout\n",
    "                or pr_soup)                        # Fallback\n",
    "        clean_text = body.get_text(\" \", strip=True)\n",
    "        txt_file.write_text(clean_text, encoding=\"utf-8\")\n",
    "    else:\n",
    "        clean_text = txt_file.read_text(encoding=\"utf-8\")\n",
    "    ```\n",
    "\n",
    "    * **Idempotenz**: Wenn die Datei schon existiert, wird nichts\n",
    "      erneut heruntergeladen ‚Äì das spart Zeit und Traffic.\n",
    "    * Der eigentliche Text sitzt mal in `#article`, mal in\n",
    "      `#content`. Wir probieren beide Selektoren und greifen im Zweifel\n",
    "      auf die ganze Seite zur√ºck.\n",
    "    * HTML und gereinigter Plain-Text werden getrennt gespeichert.\n",
    "\n",
    "11. **Metadaten sammeln**\n",
    "\n",
    "    ```python\n",
    "    records.append(\n",
    "        dict(\n",
    "            date=date_txt,\n",
    "            ressort=ressort,\n",
    "            title=title,\n",
    "            pr_url=pr_url,\n",
    "            filename_html=html_file.name,\n",
    "            filename_txt=txt_file.name,\n",
    "            n_tokens=len(clean_text.split())\n",
    "        )\n",
    "    )\n",
    "    ```\n",
    "\n",
    "    * Alle wesentlichen Infos ‚Äì inklusive Dateinamen und Token-Anzahl ‚Äì\n",
    "      landen in einem Dictionary, das wir sp√§ter direkt in ein\n",
    "      DataFrame gie√üen.\n",
    "\n",
    "12. **H√∂fliche Pause**\n",
    "\n",
    "    ```python\n",
    "    time.sleep(0.4)\n",
    "    ```\n",
    "\n",
    "    * 400 ms warten verringert die Gefahr, den Server zu √ºberlasten.\n",
    "\n",
    "13. **Auswertung in Pandas**\n",
    "\n",
    "    ```python\n",
    "    df = pd.DataFrame(records)\n",
    "    df.head()\n",
    "    ```\n",
    "\n",
    "    * Am Ende verwandeln wir die gesammelten Dictionaries in ein\n",
    "      `DataFrame`, um die ersten Zeilen gleich im Notebook\n",
    "      inspizieren zu k√∂nnen.\n",
    "\n",
    "So wird auf anschauliche Weise demonstriert, wie man **gezielt Teile einer\n",
    "HTML-Tabelle parst**, die Detailseiten herunterl√§dt, Text extrahiert und alles\n",
    "sauber f√ºr weitere Analysen ablegt.\n",
    "\n",
    "</div>\n",
    "\n",
    "<script>\n",
    "function myFunction() {\n",
    "  var x = document.getElementById(\"myDIV2\");\n",
    "  if (x.style.display === \"none\") {\n",
    "    x.style.display = \"block\";\n",
    "  } else {\n",
    "    x.style.display = \"none\";\n",
    "  }\n",
    "}\n",
    "</script>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36386c94-1263-49a0-9d1b-f7f0e09f1f4a",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
