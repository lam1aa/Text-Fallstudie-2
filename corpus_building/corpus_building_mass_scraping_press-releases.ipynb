{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e169cfbb-5409-4bc0-82db-413343b58e32",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "# üöÄ Massenscraping von Pressemitteilungen"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f17d594-10da-402b-83af-5bacddd33b7d",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "<div class=\"alert alert-block alert-info\"> <b> üîî Feinlernziel(e) dieses Kapitels</b>\n",
    "</br>\n",
    "Die Lernenden k√∂nnen mit Hilfe eines Jupyter Notebooks Python-Code zur Extraktion des Website-Texts ausf√ºhren.</br>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8db05ff-df64-4e47-aea4-beb1008c13d4",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## Hinweise zur Ausf√ºhrung des Notebooks\n",
    "Dieses Notebook kann auf unterschiedlichen Levels erarbeitet werden (siehe Abschnitt [\"Technische Voraussetzungen\"](../markdown/introduction_requirements)): \n",
    "1. Book-Only Mode\n",
    "2. Cloud Mode: Daf√ºr auf üöÄ klicken und z.B. in Colab ausf√ºhren.\n",
    "3. Local Mode: Daf√ºr auf Herunterladen ‚Üì klicken und \".ipynb\" w√§hlen. \n",
    "\n",
    "## √úbersicht\n",
    "\n",
    "Im Folgenden werden alle Pressemitteilungen der Berliner Staatskanzlei gescraped\n",
    "\n",
    "Daf√ºr werden folgendene Schritte durchgef√ºhrt:\n",
    "1. Wir werden die Struktur des Teils der Website untersuchen, der alle Pressemitteilungen enth√§lt.\n",
    "2. Wir werden die URL-Links zu allen Pressemitteilungen abrufen.\n",
    "3. Abschlie√üend werden wir alle Pressemitteilungen scrapen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0fb10a4-c384-4b7e-a956-2fa13cb1a34c",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "hide-cell"
    ]
   },
   "outputs": [],
   "source": [
    "# üöÄ Install libraries \n",
    "!pip install requests tqdm lxml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b5c5d7a-f727-4385-8c30-49347b73d692",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "hide-cell"
    ]
   },
   "outputs": [],
   "source": [
    "import requests, pathlib, time, re, logging, textwrap, csv\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a371973-de38-4439-8878-86b31954abea",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## 1. Abruf und Analyse der Suchseite f√ºr Pressemitteilungen"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b470b17-54f4-4ae5-8d97-581780c8b86b",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "Im Kapitel ['Aufbau des Forschungskorpus'](../corpus_collection/corpus-collection_building-our-corpus.html#aufbau-des-forschungskorpus) haben wir die Auswahl- und Filterprozesse f√ºr unser Korpus von Pressemitteilungen beschrieben. Nun geht es darum, das Korpus mithilfe von Scraping-Tools und HTML-Kenntnissen zu extrahieren. <!-- In the chapter ['Aufbau des Forschungskorpus'](../corpus_collection/corpus-collection_building-our-corpus.html#aufbau-des-forschungskorpus) we have outlined the selection & filtering process for our corpus of press releases. Now it is time to imlement scraping of tha corpus using scraping tools and knowledge of HTML. -->\n",
    "\n",
    "1. Wir wissen bereits, dass das [Suchmen√º](https://www.berlin.de/presse/pressemitteilungen/index/search) auf der Website Berlin.de gezielt die Auswahl der f√ºr uns interessanten Abteilungen erm√∂glicht. <!-- We already know that the [Search menu](https://www.berlin.de/presse/pressemitteilungen/index/search) on the Berlin.de website allows to select only the departments that interest us: -->\n",
    "   \n",
    "![selection](../book_images/selection_of_depts.png)\n",
    "\n",
    "2. Anschlie√üend k√∂nnen wir mit den [ausgew√§hlten Abteilungen und einer leeren Suchanfrage suchen](https://www.berlin.de/presse/pressemitteilungen/index/search/?searchtext=&boolean=0&startdate=&enddate=&alle-senatsverwaltungen=on&institutions%5B%5D=Presse-+und+Informationsamt+des+Landes+Berlin&institutions%5B%5D=Senatsverwaltung+f√ºr+Bildung%2C+Jugend+und+Familie&institutions%5B%5D=Senatsverwaltung+f√ºr+Finanzen&institutions%5B%5D=Senatsverwaltung+f√ºr+Inneres+und+Sport&institutions%5B%5D=Senatsverwaltung+f√ºr+Arbeit%2C+Soziales%2C+Gleichstellung%2C+Integration%2C+Vielfalt+und+Antidiskriminierung&institutions%5B%5D=Senatsverwaltung+f√ºr+Justiz+und+Verbraucherschutz&institutions%5B%5D=Senatsverwaltung+f√ºr+Kultur+und+Gesellschaftlichen+Zusammenhalt&institutions%5B%5D=Senatsverwaltung+f√ºr+Stadtentwicklung%2C+Bauen+und+Wohnen&institutions%5B%5D=Senatsverwaltung+f√ºr+Mobilit√§t%2C+Verkehr%2C+Klimaschutz+und+Umwelt&institutions%5B%5D=Senatsverwaltung+f√ºr+Wirtschaft%2C+Energie+und+Betriebe&institutions%5B%5D=Senatsverwaltung+f√ºr+Wissenschaft%2C+Gesundheit+und+Pflege&alle-bezirksamt=on&institutions%5B%5D=Bezirksamt+Charlottenburg-Wilmersdorf&institutions%5B%5D=Bezirksamt+Friedrichshain-Kreuzberg&institutions%5B%5D=Bezirksamt+Lichtenberg&institutions%5B%5D=Bezirksamt+Marzahn-Hellersdorf&institutions%5B%5D=Bezirksamt+Mitte&institutions%5B%5D=Bezirksamt+Neuk√∂lln&institutions%5B%5D=Bezirksamt+Pankow&institutions%5B%5D=Bezirksamt+Reinickendorf&institutions%5B%5D=Bezirksamt+Spandau&institutions%5B%5D=Bezirksamt+Steglitz-Zehlendorf&institutions%5B%5D=Bezirksamt+Tempelhof-Sch√∂neberg&institutions%5B%5D=Bezirksamt+Treptow-K√∂penick&alle-landesbeauftragte=on&institutions%5B%5D=Beauftragte+des+Senats+f√ºr+Integration+und+Migration&institutions%5B%5D=Beauftragter+zur+Aufarbeitung+der+SED-Diktatur&institutions%5B%5D=B√ºrger-+und+Polizeibeauftragter+des+Landes+Berlin&institutions%5B%5D=Pflegebeauftragte+des+Landes+Berlin&institutions%5B%5D=Landestierschutzbeauftragte&institutions%5B%5D=Landeswahlleitung&bt=) und so alle Pressemitteilungen dieser Abteilungen abrufen: <!-- After that we can [perform search with selected depatrmentes and an empty query](https://www.berlin.de/presse/pressemitteilungen/index/search/?searchtext=&boolean=0&startdate=&enddate=&alle-senatsverwaltungen=on&institutions%5B%5D=Presse-+und+Informationsamt+des+Landes+Berlin&institutions%5B%5D=Senatsverwaltung+f√ºr+Bildung%2C+Jugend+und+Familie&institutions%5B%5D=Senatsverwaltung+f√ºr+Finanzen&institutions%5B%5D=Senatsverwaltung+f√ºr+Inneres+und+Sport&institutions%5B%5D=Senatsverwaltung+f√ºr+Arbeit%2C+Soziales%2C+Gleichstellung%2C+Integration%2C+Vielfalt+und+Antidiskriminierung&institutions%5B%5D=Senatsverwaltung+f√ºr+Justiz+und+Verbraucherschutz&institutions%5B%5D=Senatsverwaltung+f√ºr+Kultur+und+Gesellschaftlichen+Zusammenhalt&institutions%5B%5D=Senatsverwaltung+f√ºr+Stadtentwicklung%2C+Bauen+und+Wohnen&institutions%5B%5D=Senatsverwaltung+f√ºr+Mobilit√§t%2C+Verkehr%2C+Klimaschutz+und+Umwelt&institutions%5B%5D=Senatsverwaltung+f√ºr+Wirtschaft%2C+Energie+und+Betriebe&institutions%5B%5D=Senatsverwaltung+f√ºr+Wissenschaft%2C+Gesundheit+und+Pflege&alle-bezirksamt=on&institutions%5B%5D=Bezirksamt+Charlottenburg-Wilmersdorf&institutions%5B%5D=Bezirksamt+Friedrichshain-Kreuzberg&institutions%5B%5D=Bezirksamt+Lichtenberg&institutions%5B%5D=Bezirksamt+Marzahn-Hellersdorf&institutions%5B%5D=Bezirksamt+Mitte&institutions%5B%5D=Bezirksamt+Neuk√∂lln&institutions%5B%5D=Bezirksamt+Pankow&institutions%5B%5D=Bezirksamt+Reinickendorf&institutions%5B%5D=Bezirksamt+Spandau&institutions%5B%5D=Bezirksamt+Steglitz-Zehlendorf&institutions%5B%5D=Bezirksamt+Tempelhof-Sch√∂neberg&institutions%5B%5D=Bezirksamt+Treptow-K√∂penick&alle-landesbeauftragte=on&institutions%5B%5D=Beauftragte+des+Senats+f√ºr+Integration+und+Migration&institutions%5B%5D=Beauftragter+zur+Aufarbeitung+der+SED-Diktatur&institutions%5B%5D=B√ºrger-+und+Polizeibeauftragter+des+Landes+Berlin&institutions%5B%5D=Pflegebeauftragte+des+Landes+Berlin&institutions%5B%5D=Landestierschutzbeauftragte&institutions%5B%5D=Landeswahlleitung&bt=) and retrieve all press releases belonging to these departments:-->\n",
    "\n",
    "![suchergebnisse](../book_images/suchergebnisse.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd435155-4cbf-456d-b39e-955b99b362e8",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "Wir sehen, dass die Links hier in einer Tabelle gespeichert sind. In HTML wird eine Tabelle mit dem `<table>`-Element dargestellt. Wenn wir den Quellcode dieser Seite betrachten, stellen wir fest, dass sie eine Tabelle enth√§lt, in der alle Links aufgef√ºhrt sind:\n",
    "\n",
    "![selection](../book_images/pm_table_source_html.png) \n",
    "\n",
    "Um diese Links zu durchsuchen, k√∂nnen wir die grundlegenden HTML-Abfragefunktionen der bereits bekannten Bibliothek BeautifulSoup verwenden. Das machen wir im n√§chsten Abschnitt."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd62da74-4319-486d-adee-9058dbc9f495",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## 2. Suchergebnisse scrapen und Pressemitteilungen extrahieren (auf einer Seite): "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96a31fb9-d636-476c-9cfa-a8765de496d0",
   "metadata": {},
   "source": [
    "### 2.1. Organisation der Ordnerstruktur"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f973641b-67cf-4a7c-8f28-a51b92e03e07",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# -- organise data ----------------------------------------------------\n",
    "FIRST_OUTPUT_PAGE = (\n",
    "    \"https://www.berlin.de/presse/pressemitteilungen/index/search/?searchtext=&boolean=0&startdate=&enddate=&alle-senatsverwaltungen=on&institutions%5B%5D=Presse-+und+Informationsamt+des+Landes+Berlin&institutions%5B%5D=Senatsverwaltung+f√ºr+Bildung%2C+Jugend+und+Familie&institutions%5B%5D=Senatsverwaltung+f√ºr+Finanzen&institutions%5B%5D=Senatsverwaltung+f√ºr+Inneres+und+Sport&institutions%5B%5D=Senatsverwaltung+f√ºr+Arbeit%2C+Soziales%2C+Gleichstellung%2C+Integration%2C+Vielfalt+und+Antidiskriminierung&institutions%5B%5D=Senatsverwaltung+f√ºr+Justiz+und+Verbraucherschutz&institutions%5B%5D=Senatsverwaltung+f√ºr+Kultur+und+Gesellschaftlichen+Zusammenhalt&institutions%5B%5D=Senatsverwaltung+f√ºr+Stadtentwicklung%2C+Bauen+und+Wohnen&institutions%5B%5D=Senatsverwaltung+f√ºr+Mobilit√§t%2C+Verkehr%2C+Klimaschutz+und+Umwelt&institutions%5B%5D=Senatsverwaltung+f√ºr+Wirtschaft%2C+Energie+und+Betriebe&institutions%5B%5D=Senatsverwaltung+f√ºr+Wissenschaft%2C+Gesundheit+und+Pflege&alle-bezirksamt=on&institutions%5B%5D=Bezirksamt+Charlottenburg-Wilmersdorf&institutions%5B%5D=Bezirksamt+Friedrichshain-Kreuzberg&institutions%5B%5D=Bezirksamt+Lichtenberg&institutions%5B%5D=Bezirksamt+Marzahn-Hellersdorf&institutions%5B%5D=Bezirksamt+Mitte&institutions%5B%5D=Bezirksamt+Neuk√∂lln&institutions%5B%5D=Bezirksamt+Pankow&institutions%5B%5D=Bezirksamt+Reinickendorf&institutions%5B%5D=Bezirksamt+Spandau&institutions%5B%5D=Bezirksamt+Steglitz-Zehlendorf&institutions%5B%5D=Bezirksamt+Tempelhof-Sch√∂neberg&institutions%5B%5D=Bezirksamt+Treptow-K√∂penick&alle-landesbeauftragte=on&institutions%5B%5D=Beauftragte+des+Senats+f√ºr+Integration+und+Migration&institutions%5B%5D=Beauftragter+zur+Aufarbeitung+der+SED-Diktatur&institutions%5B%5D=B√ºrger-+und+Polizeibeauftragter+des+Landes+Berlin&institutions%5B%5D=Pflegebeauftragte+des+Landes+Berlin&institutions%5B%5D=Landestierschutzbeauftragte&institutions%5B%5D=Landeswahlleitung&bt=\"\n",
    ") \n",
    "DATA_DIR   = pathlib.Path(\"../data\")         \n",
    "HTML_DIR   = DATA_DIR / \"html\"\n",
    "TXT_DIR    = DATA_DIR / \"txt\"\n",
    "HTML_DIR.mkdir(parents=True, exist_ok=True)\n",
    "TXT_DIR.mkdir(parents=True,  exist_ok=True)\n",
    "logging.basicConfig(level=logging.INFO, format=\"%(levelname)s: %(message)s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "191e4a8e-b7b2-413a-bf43-0339747c6a8e",
   "metadata": {},
   "source": [
    "### 2.2. Vorbereitung der Funktion, die die HTTP-Anfrage ausf√ºhrt und die Antwort verarbeitet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d83d1d3-86af-41db-a8db-b14b270d17bb",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# -- helper -----------------------------------------------------------------\n",
    "def get_soup(\n",
    "    url: str,\n",
    "    *,\n",
    "    max_retries: int = 3,\n",
    "    sleep_s: float = 10,\n",
    ") -> BeautifulSoup | None:\n",
    "    \"\"\"\n",
    "    L√§dt eine URL und gibt BeautifulSoup zur√ºck.\n",
    "    - Bei 404/410 wird nach *max_retries* Versuchen endg√ºltig None geliefert.\n",
    "    - Bei allen anderen Fehlern wird bis *max_retries* weiterprobiert.\n",
    "    \"\"\"\n",
    "    for attempt in range(1, max_retries + 1):\n",
    "        try:\n",
    "            r = requests.get(\n",
    "                url,\n",
    "                timeout=20,\n",
    "                headers={\"User-Agent\": \"Mozilla/5.0 (QuadrigaScraper/1.0)\"},\n",
    "            )\n",
    "        except requests.RequestException as err:\n",
    "            logging.warning(\"Netzwerkfehler %s ‚Äì Versuch %d/%d\", err, attempt, max_retries)\n",
    "            time.sleep(sleep_s)\n",
    "            continue\n",
    "\n",
    "        if r.status_code == 200:\n",
    "            return BeautifulSoup(r.text, \"lxml\")\n",
    "\n",
    "        # Dauerhafte Fehler: 404 (Not Found) oder 410 (Gone)\n",
    "        if r.status_code in (404, 410):\n",
    "            logging.info(\"‚ùå %s liefert %s ‚Äì √ºberspringe.\", url, r.status_code)\n",
    "            return None\n",
    "\n",
    "        logging.warning(\n",
    "            \"Status %s auf %s ‚Äì Versuch %d/%d, Wartezeit %ss\",\n",
    "            r.status_code, url, attempt, max_retries, sleep_s\n",
    "        )\n",
    "        time.sleep(sleep_s)\n",
    "\n",
    "    logging.error(\"üö® %s nach %d Versuchen nicht erreichbar ‚Äì √ºberspringe.\", url, max_retries)\n",
    "    return None\n",
    "\n",
    "\n",
    "def slugify(text_: str, maxlen: int = 60) -> str:\n",
    "    \"\"\"Rough filename-safe slug for headlines.\"\"\"\n",
    "    text_ = re.sub(r\"\\W+\", \"-\", text_.lower()).strip(\"-\")\n",
    "    return text_[:maxlen] or \"untitled\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78bc7f9e-a656-4082-b364-8fc1f99df5f5",
   "metadata": {},
   "source": [
    "### ü§î Was passiert in diesem Codeblock oben?\n",
    "\n",
    "Klicken Sie unten, um die Erkl√§rung zu lesen:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a49834fb-f7c2-4811-8319-369c5cbc6bb9",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "hide-cell"
    ]
   },
   "source": [
    "<!-- <button onclick=\"toggleDisplay('myDIV1')\">Was passiert hier oben? (Schritt-f√ºr-Schritt-Erkl√§rung)</button>\n",
    "\n",
    "<div id=\"myDIV1\" style=\"display:none\"> -->\n",
    "\n",
    "```{toggle} Was passiert in diesem Codeblock oben?\n",
    "\n",
    "**Was passiert in diesem Codeblock oben?**\n",
    "\n",
    "1. **`get_soup()`**\n",
    "\n",
    "   * **Ziel:** Eine URL abrufen und ‚Äì falls erfolgreich ‚Äì als `BeautifulSoup`-Objekt zur√ºckliefern.\n",
    "   * **Konfigurierbare Robustheit**\n",
    "\n",
    "     * `max_retries` (Std.: 3): Wie oft probiert das Skript es insgesamt?\n",
    "     * `sleep_s` (Std.: 10 s): Wartezeit zwischen den Versuchen (Schutz vor √úberlastung & Rate Limits).\n",
    "   * **Ablauf pro Versuch**\n",
    "\n",
    "     1. HTTP-Request mit\n",
    "\n",
    "        * 20 Sekunden Timeout und\n",
    "        * eigenem *User-Agent* ‚ÄûQuadrigaScraper/1.0‚Äú.\n",
    "     2. **Status 200** ‚Üí Seite wird geparst und sofort zur√ºckgegeben.\n",
    "     3. **Status 404 oder 410** ‚Üí gelten als endg√ºltig (‚ÄûSeite existiert nicht‚Äú). Nach bis zu `max_retries` Versuchen gibt die Funktion `None` zur√ºck und der Aufrufer kann den Datensatz √ºberspringen.\n",
    "     4. **Andere Fehler** (5xx, 429 usw.) oder Netzwerk-Ausnahmen l√∂sen eine Warnung aus. Nach `sleep_s` Sekunden wird erneut versucht, bis das Limit `max_retries` erreicht ist.\n",
    "   * **Ergebnis:**\n",
    "\n",
    "     * Erfolgreicher Abruf ‚Üí `BeautifulSoup`\n",
    "     * Dauerhafter Fehler ‚Üí `None` (der Scraper f√§hrt fort, ohne zu h√§ngen)\n",
    "\n",
    "2. **`slugify()`**\n",
    "\n",
    "   * Konvertiert beliebigen Text in einen **dateinamen-tauglichen ‚ÄúSlug‚Äù**:\n",
    "\n",
    "     1. Kleinschreibung\n",
    "     2. Nicht-alphanumerische Zeichen durch Bindestriche ersetzen\n",
    "     3. f√ºhrende/abschlie√üende Bindestriche entfernen\n",
    "     4. auf 60 Zeichen k√ºrzen\n",
    "   * Liefert bei komplett leerem Ergebnis den Fallback **‚Äûuntitled‚Äú**.\n",
    "\n",
    "\n",
    "```\n",
    "<!-- </div>\n",
    "\n",
    "<script>\n",
    "\n",
    "function toggleDisplay(id) {\n",
    "  var x = document.getElementById(id);\n",
    "  if (x.style.display === \"none\") {\n",
    "    x.style.display = \"block\";\n",
    "  } else {\n",
    "    x.style.display = \"none\";\n",
    "  }\n",
    "}\n",
    "</script> -->"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eeb1624c-c82e-493b-9dc7-3c877f4193a8",
   "metadata": {},
   "source": [
    "### 2.3. Extrahieren von Pressemitteilungen aus einer Seite mit Suchausgabe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "577fdf92-4f28-4a8a-8dfa-5c607098d6a9",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# -- Schritt 1: genau **eine** Ergebnisseite parsen -----------------------\n",
    "search_soup = get_soup(FIRST_OUTPUT_PAGE)\n",
    "\n",
    "rows = search_soup.select(\"table tbody tr\")\n",
    "records = []\n",
    "print(f\"Found {len(rows)} rows on the page.\")\n",
    "\n",
    "for tr in tqdm(rows, desc=\"Rows\"):\n",
    "    # alle <td>-Zellen der Zeile auf einmal holen\n",
    "    cells = tr.find_all(\"td\")\n",
    "    if len(cells) < 3:          # Fu√üzeilen / Leerzeilen √ºberspringen\n",
    "        continue\n",
    "\n",
    "    # Spalte 1 ‚Äì Datum\n",
    "    date_txt = cells[0].get_text(strip=True)\n",
    "\n",
    "    # Spalte 2 ‚Äì √úberschrift + Link\n",
    "    anchor = cells[1].find(\"a\", href=True)\n",
    "    if anchor is None:          # Sicherheits-Check\n",
    "        continue\n",
    "    title   = anchor.get_text(strip=True)\n",
    "    pr_url  = \"https://www.berlin.de\" + anchor[\"href\"]\n",
    "\n",
    "    # Spalte 3 ‚Äì herausgebende Beh√∂rde (‚ÄûRessort‚Äú)\n",
    "    ressort = cells[2].get_text(strip=True)\n",
    "\n",
    "    # deterministische ID, z. B. 1570469\n",
    "    uid = anchor[\"href\"].split(\"/\")[-1].split(\".\")[-2]\n",
    "\n",
    "        # -- Schritt 2: Detailseite herunterladen -------------------------\n",
    "    html_file = HTML_DIR / f\"{uid}.html\"\n",
    "    txt_file  = TXT_DIR  / f\"{uid}.txt\"\n",
    "\n",
    "    if not html_file.exists():        # √ºberspringen, wenn bereits gescrapet\n",
    "        pr_soup = get_soup(pr_url)\n",
    "\n",
    "        # rohe HTML-Datei speichern\n",
    "        html_file.write_text(str(pr_soup), encoding=\"utf-8\")\n",
    "\n",
    "        # Haupttext extrahieren; Fallback, falls sich die ID √§ndert\n",
    "        body = (pr_soup.select_one(\"#article\") or        # neues Layout (2024)\n",
    "                pr_soup.select_one(\"#content\") or        # klassisches Layout\n",
    "                pr_soup)                                 # letzter Ausweg\n",
    "        clean_text = body.get_text(\" \", strip=True)\n",
    "        txt_file.write_text(clean_text, encoding=\"utf-8\")\n",
    "    else:\n",
    "        # F√ºr das DataFrame trotzdem L√§nge des Textes bestimmen\n",
    "        clean_text = txt_file.read_text(encoding=\"utf-8\")\n",
    "\n",
    "    records.append(\n",
    "        dict(\n",
    "            date=date_txt,\n",
    "            ressort=ressort,\n",
    "            title=title,\n",
    "            pr_url=pr_url,\n",
    "            filename_html=html_file.name,\n",
    "            filename_txt=txt_file.name,\n",
    "            n_tokens=len(clean_text.split())\n",
    "        )\n",
    "    )\n",
    "    time.sleep(0.4)      # H√∂flichkeitspause\n",
    "\n",
    "# -- Schritt 3: Metadaten inspizieren ------------------------------------\n",
    "df = pd.DataFrame(records)\n",
    "df.head()          # normale Jupyter-Ausgabe gen√ºgt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f412907d-d3d2-43be-a0ff-1cad98dc3be1",
   "metadata": {},
   "source": [
    "### ü§î Was passiert in diesem Codeblock oben? \n",
    "Klicken Sie unten, um die Schritt-f√ºr-Schritt-Erkl√§rung zu lesen: "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cba200c-4ff3-4606-8ad1-06814b431af2",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "hide-cell"
    ]
   },
   "source": [
    "<!-- <button onclick=\"toggleDisplay('myDIV2')\">Was passiert hier oben? (Schritt-f√ºr-Schritt-Erkl√§rung)</button>\n",
    "\n",
    "<div id=\"myDIV2\" style=\"display:none\"> -->\n",
    "\n",
    "\n",
    "```{toggle} Was passiert in diesem Codeblock oben?\n",
    "\n",
    "**Was passiert in diesem Codeblock oben? ‚Äì Schritt f√ºr Schritt**\n",
    "\n",
    "1. **Erste Seite einlesen**\n",
    "\n",
    "   `search_soup = get_soup(SAMPLE_OUTPUT_PAGE)\n",
    "   rows = search_soup.select(\"table tbody tr\")`\n",
    "\n",
    "   * Die Funktion `get_soup()` l√§dt genau **eine** Ergebnisseite der\n",
    "     Such-/Listenansicht und gibt sie als Beautiful-Soup-Objekt zur√ºck.\n",
    "   * Mit dem CSS-Selektor `table tbody tr` werden alle Tabellenzeilen\n",
    "     der Ergebnisliste eingesammelt. Jede Zeile repr√§sentiert eine\n",
    "     einzelne Pressemitteilung.\n",
    "\n",
    "2. **Vorbereitung f√ºr die Schleife**\n",
    "\n",
    "   `records = []\n",
    "   print(f\"Found {len(rows)} rows on the page.\")`\n",
    "\n",
    "   * `records` soll sp√§ter eine Liste von Dictionaries f√ºr das\n",
    "     DataFrame sammeln.\n",
    "   * Eine kurze Ausgabe zeigt, wie viele Zeilen tats√§chlich gefunden\n",
    "     wurden ‚Äì n√ºtzlich f√ºr Kontroll-/Debug-Zwecke.\n",
    "\n",
    "3. **Iterieren mit Fortschrittsbalken**\n",
    "\n",
    "   `for tr in tqdm(rows, desc=\"Rows\"):`\n",
    "\n",
    "   * `tqdm` liefert einen h√ºbschen Fortschrittsbalken ‚Äì perfekt f√ºr\n",
    "     Lehr- und Live-Demos.\n",
    "\n",
    "4. **Zellen extrahieren & Plausibilit√§t pr√ºfen**\n",
    "\n",
    "   `\n",
    "   cells = tr.find_all(\"td\")\n",
    "   if len(cells) < 3:          # footer / empty rows ‚Üí ignore\n",
    "       continue\n",
    "   `\n",
    "\n",
    "   * Alle `<td>` einer Zeile werden auf einmal geholt.\n",
    "   * Hat eine Zeile weniger als drei Zellen, handelt es sich um\n",
    "     Paginierungs- oder Leerzeilen; die werden √ºbersprungen.\n",
    "\n",
    "5. **Spalte 1 ‚Äì Datum**\n",
    "\n",
    "   `\n",
    "   date_txt = cells[0].get_text(strip=True)\n",
    "   `\n",
    "\n",
    "   * `strip=True` entfernt Zeilenumbr√ºche und Leerzeichen ‚Äì wir erhalten\n",
    "     saubere Strings wie ‚Äû16.06.2025‚Äú.\n",
    "\n",
    "6. **Spalte 2 ‚Äì √úberschrift & Link**\n",
    "\n",
    "   `\n",
    "   anchor = cells[1].find(\"a\", href=True)\n",
    "   if anchor is None:\n",
    "       continue\n",
    "   title  = anchor.get_text(strip=True)\n",
    "   pr_url = \"https://www.berlin.de\" + anchor[\"href\"]\n",
    "   `\n",
    "\n",
    "   * Innerhalb der zweiten Zelle steckt der anklickbare Link.\n",
    "   * Sicherheits-Check: Falls doch kein `<a>` vorhanden ist, Zeile\n",
    "     √ºberspringen.\n",
    "   * Die relative URL wird zur vollst√§ndigen URL erg√§nzt.\n",
    "\n",
    "7. **Spalte 3 ‚Äì Ressort (herausgebende Beh√∂rde)**\n",
    "\n",
    "   `\n",
    "   ressort = cells[2].get_text(strip=True)\n",
    "   `\n",
    "\n",
    "8. **Eindeutige ID ableiten**\n",
    "\n",
    "   `\n",
    "   uid = anchor[\"href\"].split(\"/\")[-1].split(\".\")[-2]\n",
    "   `\n",
    "\n",
    "   * Vom Pfadsegment `pressemitteilung.1570469.php` wird mittels\n",
    "     `split()` das numerische St√ºck **1570469** herausgel√∂st.\n",
    "   * Diese ID landet sp√§ter im Dateinamen, damit jeder Release genau\n",
    "     eine HTML- und eine TXT-Datei bekommt.\n",
    "\n",
    "9. **Dateipfade festlegen**\n",
    "\n",
    "   `\n",
    "   html_file = HTML_DIR / f\"{uid}.html\"\n",
    "   txt_file  = TXT_DIR  / f\"{uid}.txt\"\n",
    "   `\n",
    "\n",
    "10. **HTML herunterladen & Text extrahieren (nur falls neu)**\n",
    "\n",
    "    `\n",
    "    if not html_file.exists():\n",
    "        pr_soup = get_soup(pr_url)\n",
    "        html_file.write_text(str(pr_soup), encoding=\"utf-8\")\n",
    "\n",
    "        body = (pr_soup.select_one(\"#article\")     # neues Layout\n",
    "                or pr_soup.select_one(\"#content\")  # altes Layout\n",
    "                or pr_soup)                        # Fallback\n",
    "        clean_text = body.get_text(\" \", strip=True)\n",
    "        txt_file.write_text(clean_text, encoding=\"utf-8\")\n",
    "    else:\n",
    "        clean_text = txt_file.read_text(encoding=\"utf-8\")\n",
    "    `\n",
    "\n",
    "    * **Idempotenz**: Wenn die Datei schon existiert, wird nichts\n",
    "      erneut heruntergeladen ‚Äì das spart Zeit und Traffic.\n",
    "    * Der eigentliche Text sitzt mal in `#article`, mal in\n",
    "      `#content`. Wir probieren beide Selektoren und greifen im Zweifel\n",
    "      auf die ganze Seite zur√ºck.\n",
    "    * HTML und gereinigter Plain-Text werden getrennt gespeichert.\n",
    "\n",
    "11. **Metadaten sammeln**\n",
    "\n",
    "    `\n",
    "    records.append(\n",
    "        dict(\n",
    "            date=date_txt,\n",
    "            ressort=ressort,\n",
    "            title=title,\n",
    "            pr_url=pr_url,\n",
    "            filename_html=html_file.name,\n",
    "            filename_txt=txt_file.name,\n",
    "            n_tokens=len(clean_text.split())\n",
    "        )\n",
    "    )\n",
    "    `\n",
    "\n",
    "    * Alle wesentlichen Infos ‚Äì inklusive Dateinamen und Token-Anzahl ‚Äì\n",
    "      landen in einem Dictionary, das wir sp√§ter direkt in ein\n",
    "      DataFrame gie√üen.\n",
    "\n",
    "12. **H√∂fliche Pause**\n",
    "\n",
    "    `time.sleep(0.4)`\n",
    "\n",
    "    * 400 ms warten verringert die Gefahr, den Server zu √ºberlasten.\n",
    "\n",
    "13. **Auswertung in Pandas**\n",
    "\n",
    "    \n",
    "    `df = pd.DataFrame(records)`\n",
    "    `df.head()`\n",
    "\n",
    "    * Am Ende verwandeln wir die gesammelten Dictionaries in ein\n",
    "      `DataFrame`, um die ersten Zeilen gleich im Notebook\n",
    "      inspizieren zu k√∂nnen.\n",
    "\n",
    "So wird auf anschauliche Weise demonstriert, wie man **gezielt Teile einer\n",
    "HTML-Tabelle parst**, die Detailseiten herunterl√§dt, Text extrahiert und alles\n",
    "sauber f√ºr weitere Analysen ablegt.\n",
    "```\n",
    "\n",
    "<!-- </div> -->\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b86ad42-b2d0-49a5-9c83-f2bb151f5f01",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## 3. Massenscraping von Pressemitteilungen"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b867c214-0ce2-4102-aa0e-da7f4e93e9f2",
   "metadata": {},
   "source": [
    "### 3.1. Implementierung der Logik zum Finden der letzten Seite in der Suchausgabe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d489611-65f5-4542-96ff-e1d2b41fa71d",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ‚ïî‚ïê‚ï° 80_pagination_helpers\n",
    "# Basis-URL ohne \"/page/<nr>\"-Segment,\n",
    "# aber *inklusive* aller Query-Parameter (= Filter der langen Such-URL)\n",
    "\n",
    "SEARCH_ROOT = FIRST_OUTPUT_PAGE\n",
    "\n",
    "def last_page_number() -> int:\n",
    "    \"\"\"\n",
    "    Ermittelt √ºber das <nav>-Element ('pager-skip-to-last') die h√∂chste\n",
    "    Ergebnisseiten-Nummer.\n",
    "    \"\"\"\n",
    "    soup = get_soup(SEARCH_ROOT)                            # 1. Suchseite laden\n",
    "    last_link = soup.select_one(\"li.pager-skip-to-last a\")\n",
    "    if not last_link:\n",
    "        raise RuntimeError(\"Konnte die letzte Seite nicht finden ‚Äì Selector?\")\n",
    "    \n",
    "    # href hat die Form \".../page/5239?<query>\"\n",
    "    m = re.search(r\"/page/(\\d+)\", last_link[\"href\"])\n",
    "    if not m:\n",
    "        raise RuntimeError(\"Seitenzahl nicht im href gefunden.\")\n",
    "    return int(m.group(1))\n",
    "\n",
    "def page_url(page_num: int) -> str:\n",
    "    \"\"\"\n",
    "    Baut die URL f√ºr eine beliebige Seite nach folgendem Muster auf:\n",
    "    <root>/page/<nr>?<identische Query-Parameter>\n",
    "    \"\"\"\n",
    "    if page_num < 1:\n",
    "        raise ValueError(\"Seitennummern beginnen bei 1.\")\n",
    "    return SEARCH_ROOT.replace(\"/search/\", f\"/search/page/{page_num}/\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8a7f5f7-cc23-4243-a0df-80536dbe1994",
   "metadata": {},
   "source": [
    "### 3.2. Vorbereitung der Funktion f√ºr das Massenscraping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76d94122-daf5-4ef4-9790-e44122ce6e1f",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ‚ïî‚ïê‚ï° 81_bulk_crawler_with_autosave\n",
    "META_CSV   = DATA_DIR / \"metadata.csv\"\n",
    "BUFFER_SIZE = 100          # wie viele Records, bevor wir in die CSV fluschen?\n",
    "\n",
    "def crawl_all_pages(\n",
    "    pages: int | None = None,\n",
    "    sleep_s: float = 0.4,\n",
    "    buffer_size: int = BUFFER_SIZE,\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"Crawlt alle Trefferseiten, speichert Metadaten inkrementell nach CSV.\"\"\"\n",
    "    if pages is None:\n",
    "        pages = last_page_number()\n",
    "        print(f\"‚Üí Letzte Trefferseite lautet {pages}\")\n",
    "\n",
    "    # --------------------------------------------------------------\n",
    "    EXPECTED_HEADER = [\"id\", \"url\", \"date\", \"title\",\n",
    "                       \"source\", \"filename_html\", \"filename\", \"n_tokens\"]\n",
    "    \n",
    "    existing_uids: set[str] = set()\n",
    "    need_header = True                                           # default\n",
    "    \n",
    "    if META_CSV.exists() and META_CSV.stat().st_size > 0:\n",
    "        # Pr√ºfen, ob schon ein Header vorhanden ist\n",
    "        with open(META_CSV, newline=\"\", encoding=\"utf-8\") as fh:\n",
    "            first_line = fh.readline().strip()\n",
    "            need_header = first_line.split(\",\") != EXPECTED_HEADER\n",
    "    \n",
    "        # UIDs nur einlesen, wenn Header vorhanden\n",
    "        if not need_header:\n",
    "            with open(META_CSV, newline=\"\", encoding=\"utf-8\") as fh:\n",
    "                reader = csv.DictReader(fh)\n",
    "                existing_uids = {row[\"id\"] for row in reader}\n",
    "                logging.info(\"‚öôÔ∏è  %d vorhandene Datens√§tze erkannt.\", len(existing_uids))\n",
    "        else:\n",
    "            logging.warning(\"‚òùÔ∏è  metadata.csv hat noch keinen Header ‚Äì wird erg√§nzt.\")\n",
    "    \n",
    "    # √ñffnen im richtigen Modus\n",
    "    mode = \"a\" if META_CSV.exists() else \"w\"\n",
    "    csvfile = open(META_CSV, mode, newline=\"\", encoding=\"utf-8\")\n",
    "    writer = csv.DictWriter(csvfile, fieldnames=EXPECTED_HEADER)\n",
    "    \n",
    "    if need_header:\n",
    "        writer.writeheader()\n",
    "        csvfile.flush()\n",
    "\n",
    "    buffer: list[dict] = [] \n",
    "    # --------------------------------------------------------------\n",
    "\n",
    "\n",
    "    for p in tqdm(range(1, pages + 1), desc=\"Result pages\"):\n",
    "        list_soup = get_soup(page_url(p))\n",
    "        if list_soup is None:\n",
    "            continue                                # ganze Seite √ºberspringen\n",
    "\n",
    "        for tr in list_soup.select(\"table tbody tr\"):\n",
    "            cells = tr.find_all(\"td\")\n",
    "            if len(cells) < 3:\n",
    "                continue\n",
    "\n",
    "            date_txt = cells[0].get_text(strip=True)\n",
    "            anchor   = cells[1].find(\"a\", href=True)\n",
    "            if anchor is None:\n",
    "                continue\n",
    "            ressort  = cells[2].get_text(strip=True)\n",
    "            pr_url   = \"https://www.berlin.de\" + anchor[\"href\"]\n",
    "            uid      = anchor[\"href\"].split(\".\")[-2]\n",
    "\n",
    "            # Doppelte auslassen (wichtig f√ºr Resume!)\n",
    "            if uid in existing_uids:\n",
    "                continue # Datensatz bereits vorhanden ‚Üí √ºberspringen\n",
    "\n",
    "            html_fp = HTML_DIR / f\"{uid}.html\"\n",
    "            txt_fp  = TXT_DIR  / f\"{uid}.txt\"\n",
    "\n",
    "            # -------- Detailseite laden (oder bei 404 √ºberspringen) ----------\n",
    "            pr_soup = get_soup(pr_url)\n",
    "            if pr_soup is None:                   # 404 ‚Üí keinen Record anlegen\n",
    "                continue\n",
    "\n",
    "            if not html_fp.exists():\n",
    "                html_fp.write_text(str(pr_soup), encoding=\"utf-8\")\n",
    "                body = (pr_soup.select_one(\"#article\") or\n",
    "                        pr_soup.select_one(\"#content\") or\n",
    "                        pr_soup)\n",
    "                clean = body.get_text(\" \", strip=True)\n",
    "                txt_fp.write_text(clean, encoding=\"utf-8\")\n",
    "            else:\n",
    "                clean = txt_fp.read_text(encoding=\"utf-8\")\n",
    "\n",
    "            rec = dict(\n",
    "                id=uid,\n",
    "                url=pr_url,\n",
    "                date=date_txt,\n",
    "                title=anchor.get_text(strip=True),\n",
    "                source=ressort,\n",
    "                filename_html=html_fp.name,\n",
    "                filename=txt_fp.name,        # nur ‚Äûfilename‚Äú\n",
    "                n_tokens=len(clean.split())\n",
    "            )\n",
    "            buffer.append(rec)\n",
    "            existing_uids.add(uid)                # Direkt markieren\n",
    "            time.sleep(sleep_s)\n",
    "\n",
    "            # ---------- Zwischenspeichern ----------------------------------\n",
    "            if len(buffer) >= buffer_size:\n",
    "                writer.writerows(buffer)\n",
    "                csvfile.flush()\n",
    "                buffer.clear()\n",
    "\n",
    "        # Optional: Auch nach *jeder* Seite flushen\n",
    "        if buffer:\n",
    "            writer.writerows(buffer)\n",
    "            csvfile.flush()\n",
    "            buffer.clear()\n",
    "\n",
    "    csvfile.close()\n",
    "    logging.info(\"‚úÖ Crawl abgeschlossen, CSV geschlossen.\")\n",
    "\n",
    "    # Finales DataFrame (f√ºr direkte Notebook-Analyse)\n",
    "    return pd.read_csv(META_CSV, encoding=\"utf-8\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "291bc1f4-70bc-4294-9c11-48912b62c391",
   "metadata": {},
   "source": [
    "### ü§î Was passiert in diesem Codeblock oben? \n",
    "Klicken Sie unten, um die Schritt-f√ºr-Schritt-Erkl√§rung zu lesen: "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2352d104-742a-4548-a71a-dffc143e20b4",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "```{toggle}\n",
    "\n",
    "**Was passiert in diesem Bulk-Crawler oben?**\n",
    "\n",
    "1. **Einstiegs-Parameter**\n",
    "\n",
    "   * `pages` ‚Äì wie viele Trefferseiten sollen verarbeitet werden? `None` ruft zuerst `last_page_number()` auf.\n",
    "   * `sleep_s` ‚Äì H√∂flichkeits-Delay zwischen den Detail-Requests.\n",
    "   * `buffer_size` ‚Äì Zahl der Datens√§tze, die gepuffert werden, bevor sie in die CSV geschrieben werden.\n",
    "\n",
    "2. **CSV-Vorbereitung**\n",
    "\n",
    "   * Erwarteter Kopf: `[\"id\",\"url\",\"date\",\"title\",\"source\",\"filename_html\",\"filename\",\"n_tokens\"]`.\n",
    "   * Wenn `metadata.csv` existiert, wird gepr√ºft, ob bereits ein Header vorhanden ist.\n",
    "   * Sind schon Daten da, werden alle vorhandenen UIDs in die Menge `existing_uids` geladen, damit nichts doppelt gescrapet wird.\n",
    "\n",
    "3. **CSV-Writer & Puffer**\n",
    "\n",
    "   * Datei-Modus: `'a'`, falls die Datei schon da ist, sonst `'w'`.\n",
    "   * Header wird geschrieben, falls er noch fehlt.\n",
    "   * `buffer` sammelt neue Datens√§tze, bis `buffer_size` erreicht ist.\n",
    "\n",
    "4. **Schleife √ºber Ergebnis-Seiten**\n",
    "\n",
    "   * `for p in tqdm(range(1, pages + 1), desc=\"Result pages\"):` zeigt einen Fortschrittsbalken.\n",
    "   * `get_soup(page_url(p))` holt die jeweilige HTML-Seite; liefert sie `None`, wird die ganze Seite √ºbersprungen.\n",
    "\n",
    "5. **Tabellenzeilen auswerten**\n",
    "\n",
    "   * Jede Zeile liefert `date_txt`, `anchor` (Link & Titel) und `ressort`.\n",
    "   * UID wird aus dem Link extrahiert: `uid = anchor[\"href\"].split(\".\")[-2]`.\n",
    "   * Wenn `uid in existing_uids`, wird sofort weitergemacht (`continue`).\n",
    "\n",
    "6. **Detailseite holen & speichern**\n",
    "\n",
    "   * HTML-Datei-Pfad: `html_fp = HTML_DIR / f\"{uid}.html\"`.\n",
    "   * TXT-Datei-Pfad: `txt_fp = TXT_DIR  / f\"{uid}.txt\"`.\n",
    "   * Neue Detailseite wird nur geladen, wenn `html_fp` noch nicht existiert.\n",
    "   * Text wird aus `#article`, `#content` oder notfalls dem ganzen Dokument extrahiert.\n",
    "\n",
    "7. **Metadaten-Record bauen**\n",
    "\n",
    "   * Beispiel-Dict:\n",
    "     `{\"id\":uid, \"url\":pr_url, \"date\":date_txt, \"title\":title, \"source\":ressort, \"filename_html\":html_fp.name, \"filename\":txt_fp.name, \"n_tokens\":len(clean.split())}`\n",
    "   * Record landet im `buffer`, UID wird gleichzeitig zu `existing_uids` hinzugef√ºgt.\n",
    "   * `time.sleep(sleep_s)` wahrt Server-H√∂flichkeit.\n",
    "\n",
    "8. **Autosave-Mechanismus**\n",
    "\n",
    "   * `if len(buffer) >= buffer_size:` ‚Üí `writer.writerows(buffer)` schreibt den Puffer in die CSV, danach `buffer.clear()`.\n",
    "   * Zus√§tzlich wird nach jeder fertigen Ergebnisseite geflusht, damit h√∂chstens eine Seite verloren gehen kann.\n",
    "\n",
    "9. **Aufr√§umen & R√ºckgabe**\n",
    "\n",
    "   * Nach der gro√üen Schleife: `csvfile.close()` und ein Log-Eintrag `‚úÖ Crawl abgeschlossen`.\n",
    "   * Zum Schluss wird die komplette `metadata.csv` per `pd.read_csv()` als DataFrame zur√ºckgegeben, sodass man direkt im Notebook weiterarbeiten kann.\n",
    "\n",
    "**Kurz gesagt:** Der Crawler verarbeitet beliebig viele Trefferseiten, speichert Metadaten inkrementell, √ºberspringt 404/410-Links und setzt einen unterbrochenen Lauf dank `existing_uids` nahtlos fort.\n",
    "\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c4a76db-0e7d-4b32-8e97-6b4f85b43231",
   "metadata": {},
   "source": [
    "### 3.3. Massenscraping"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5353fee-8496-417f-9d7f-3d1cde64dc9c",
   "metadata": {},
   "source": [
    "#### Test run:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8472519c-13bc-454e-9e8a-06a727816374",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "skip-execution"
    ]
   },
   "outputs": [],
   "source": [
    "# ‚ïî‚ïê‚ï° 82_test_run_5_pages\n",
    "df_test = crawl_all_pages(pages=5)\n",
    "df_test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef9a2448-bfa5-4380-91dc-8036b197d8fb",
   "metadata": {},
   "source": [
    "#### Main run: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "752fee21-6192-47a2-812b-ebc10e814bb8",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "skip-execution"
    ]
   },
   "outputs": [],
   "source": [
    "# ‚ïî‚ïê‚ï° 83_run_bulk\n",
    "# Achtung: Das kann > 1 Stunde dauern und tausende Dateien erzeugen!\n",
    "df_all = crawl_all_pages(pages=None)   # None ‚Üí auto-detect\n",
    "df_all.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a9fddbe-b359-4aa3-a175-7a040c7b69d1",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "```{toggle}\n",
    "### Hilfscode zum Entfernen unn√∂tiger Teile des HTML-Textes\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1595724-c4ea-46d0-9c73-9fa89d939a9b",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "skip-execution",
     "hide-cell"
    ]
   },
   "outputs": [],
   "source": [
    "## 84 Hilfscode zum Entfernen unn√∂tiger Teile des HTML-Textes\n",
    "\n",
    "BACKUP_DIR = DATA_DIR / \"txt_backup\"    # Sicherheitskopien\n",
    "BACKUP_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "def extract_release(soup: BeautifulSoup) -> str | None:\n",
    "    \"\"\"Gibt den bereinigten Text einer Pressemitteilung zur√ºck (oder None).\"\"\"\n",
    "    h1   = soup.select_one(\"#layout-grid__area--herounit h1, h1.title\")\n",
    "    main = soup.select_one(\"#layout-grid__area--maincontent\")\n",
    "    if not main:\n",
    "        return None\n",
    "\n",
    "    title = h1.get_text(\" \", strip=True) if h1 else \"\"\n",
    "    body  = main.get_text(\" \", strip=True)\n",
    "\n",
    "    # Doppelte √úberschrift am Anfang entfernen\n",
    "    if body.lower().startswith(title.lower()):\n",
    "        body = body[len(title):].lstrip()\n",
    "\n",
    "    return (title + \"\\n\\n\" + body).strip()\n",
    "\n",
    "changed = skipped = failed = 0\n",
    "\n",
    "for html_fp in tqdm(sorted(HTML_DIR.glob(\"*.html\")), desc=\"Rebuild\"):\n",
    "    soup = BeautifulSoup(html_fp.read_text(encoding=\"utf-8\"), \"lxml\")\n",
    "    text = extract_release(soup)\n",
    "\n",
    "    if text is None:\n",
    "        failed += 1\n",
    "        logging.warning(\"‚ö†Ô∏è  %s: kein maincontent-Div gefunden\", html_fp.name)\n",
    "        continue\n",
    "\n",
    "    txt_fp = TXT_DIR / f\"{html_fp.stem}.txt\"\n",
    "    if txt_fp.exists() and txt_fp.read_text(encoding=\"utf-8\").strip() == text:\n",
    "        skipped += 1\n",
    "        continue\n",
    "\n",
    "    # Sicherheitskopie der alten Datei\n",
    "    if txt_fp.exists():\n",
    "        txt_fp.rename(BACKUP_DIR / txt_fp.name)\n",
    "\n",
    "    txt_fp.write_text(text, encoding=\"utf-8\")\n",
    "    changed += 1\n",
    "\n",
    "print(f\"‚úîÔ∏è  {changed:,} Dateien bereinigt ‚Äì {skipped:,} bereits ok ‚Äì \"\n",
    "      f\"{failed:,} ohne passenden Layout-Block\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d0aacf8-aa2f-4d37-9767-96b677d00027",
   "metadata": {},
   "source": [
    "## 4. Ergebnisse\n",
    "\n",
    "Der Dataframe 'df_all' und die Datei metadata.csv enthalten nun die Metadaten f√ºr das Pressemitteilungskorpus, w√§hrend der Ordner txt die Texte der Pressemitteilungen enth√§lt. Sie k√∂nnen nun mit computergest√ºtzten Methoden untersucht werden.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3d6bf41-cb38-4cc0-8330-2286f3b30b14",
   "metadata": {},
   "source": [
    "## 5. Zusammenfassung des technischen Workflow\n",
    "\n",
    "1. **Filter setzen** im Online‚ÄëFormular ‚Üí alle oben aufgef√ºhrten Institutionen anhaken (siehe URL in der Notebook‚ÄëKonstante `SEARCH_ROOT`).\n",
    "2. **Letzte Ergebnisseite ermitteln** via CSS‚ÄëSelektor `li.pager-skip-to-last a` (Stand¬†Juni‚ÄØ2025: Seite¬†5239).\n",
    "3. **Pagination ablaufen**\n",
    "\n",
    "   * Trefferzeilen auslesen (`table tbody tr`).\n",
    "   * Detailseiten abrufen; Haupttext steckt verl√§sslich in `#layout-grid__area--maincontent` (Fallback: `#article` oder `#content`).\n",
    "   * HTML¬†+¬†bereinigter Plain‚ÄëText unter `data/html/<id>.html` bzw. `data/txt/<id>.txt` speichern.\n",
    "   * Metadaten (Datum, Titel, Ressort, Dateinamen, Token‚ÄëZahl) inkrementell in `data/metadata.csv` anh√§ngen (Autosave alle¬†100¬†Datens√§tze).\n",
    "4. **Resume‚ÄëF√§higkeit**: Vor jedem Lauf werden vorhandene UIDs aus der CSV eingelesen¬†‚Üí keine Dubletten, unterbrochene Crawls lassen sich fortsetzen.\n",
    "\n",
    "```{note}\n",
    "404‚ÄëSeiten werden nach drei Fehlversuchen √ºbersprungen und im Log markiert.\n",
    "```\n",
    "\n",
    "### Korpusumfang (23.‚ÄØ06.‚ÄØ2025)\n",
    "\n",
    "* Pressemitteilungen: **‚âà‚ÄØ51‚ÄØ800**\n",
    "* Zeitspanne: 2001¬†‚Äì¬†2025\n",
    "* √ò¬†L√§nge: 430¬†Tokens¬†(Median¬†394)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
