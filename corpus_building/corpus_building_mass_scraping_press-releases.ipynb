{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e169cfbb-5409-4bc0-82db-413343b58e32",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "# üöÄ Massenscraping von Pressemitteilungen"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f17d594-10da-402b-83af-5bacddd33b7d",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "<div class=\"alert alert-block alert-info\"> <b> üîî Feinlernziel(e) dieses Kapitels</b>\n",
    "</br>\n",
    "Die Lernenden k√∂nnen mit Hilfe eines Jupyter Notebooks Python-Code zur Extraktion des Website-Texts ausf√ºhren.</br>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8db05ff-df64-4e47-aea4-beb1008c13d4",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## Hinweise zur Ausf√ºhrung des Notebooks\n",
    "Dieses Notebook kann auf unterschiedlichen Levels erarbeitet werden (siehe Abschnitt [\"Technische Voraussetzungen\"](../markdown/introduction_requirements)): \n",
    "1. Book-Only Mode\n",
    "2. Cloud Mode: Daf√ºr auf üöÄ klicken und z.B. in Colab ausf√ºhren.\n",
    "3. Local Mode: Daf√ºr auf Herunterladen ‚Üì klicken und \".ipynb\" w√§hlen. \n",
    "\n",
    "## √úbersicht\n",
    "\n",
    "Im Folgenden werden alle Pressemitteilungen der Berliner Staatskanzlei gescraped\n",
    "\n",
    "Daf√ºr werden folgendene Schritte durchgef√ºhrt:\n",
    "1. Wir werden die Struktur des Teils der Website untersuchen, der alle Pressemitteilungen enth√§lt.\n",
    "2. Wir werden die URL-Links zu allen Pressemitteilungen abrufen.\n",
    "3. Abschlie√üend werden wir alle Pressemitteilungen scrapen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0fb10a4-c384-4b7e-a956-2fa13cb1a34c",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "hide-cell"
    ]
   },
   "outputs": [],
   "source": [
    "# üöÄ Install libraries \n",
    "!pip install requests tqdm "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b5c5d7a-f727-4385-8c30-49347b73d692",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "hide-cell"
    ]
   },
   "outputs": [],
   "source": [
    "import requests, pathlib, time, re, logging, textwrap\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a371973-de38-4439-8878-86b31954abea",
   "metadata": {},
   "source": [
    "## Abruf und Analyse der Suchseite f√ºr Pressemitteilungen"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b470b17-54f4-4ae5-8d97-581780c8b86b",
   "metadata": {},
   "source": [
    "Im Kapitel ['Aufbau des Forschungskorpus'](../corpus_collection/corpus-collection_building-our-corpus.html#aufbau-des-forschungskorpus) haben wir die Auswahl- und Filterprozesse f√ºr unser Korpus von Pressemitteilungen beschrieben. Nun geht es darum, das Korpus mithilfe von Scraping-Tools und HTML-Kenntnissen zu extrahieren. <!-- In the chapter ['Aufbau des Forschungskorpus'](../corpus_collection/corpus-collection_building-our-corpus.html#aufbau-des-forschungskorpus) we have outlined the selection & filtering process for our corpus of press releases. Now it is time to imlement scraping of tha corpus using scraping tools and knowledge of HTML. -->\n",
    "\n",
    "1. Wir wissen bereits, dass das [Suchmen√º](https://www.berlin.de/presse/pressemitteilungen/index/search) auf der Website Berlin.de gezielt die Auswahl der f√ºr uns interessanten Abteilungen erm√∂glicht. <!-- We already know that the [Search menu](https://www.berlin.de/presse/pressemitteilungen/index/search) on the Berlin.de website allows to select only the departments that interest us: -->\n",
    "   \n",
    "![selection](../book_images/selection_of_depts.png)\n",
    "\n",
    "2. Anschlie√üend k√∂nnen wir mit den [ausgew√§hlten Abteilungen und einer leeren Suchanfrage suchen](https://www.berlin.de/presse/pressemitteilungen/index/search/?searchtext=&boolean=0&startdate=&enddate=&alle-senatsverwaltungen=on&institutions%5B%5D=Presse-+und+Informationsamt+des+Landes+Berlin&institutions%5B%5D=Senatsverwaltung+f√ºr+Bildung%2C+Jugend+und+Familie&institutions%5B%5D=Senatsverwaltung+f√ºr+Finanzen&institutions%5B%5D=Senatsverwaltung+f√ºr+Inneres+und+Sport&institutions%5B%5D=Senatsverwaltung+f√ºr+Arbeit%2C+Soziales%2C+Gleichstellung%2C+Integration%2C+Vielfalt+und+Antidiskriminierung&institutions%5B%5D=Senatsverwaltung+f√ºr+Justiz+und+Verbraucherschutz&institutions%5B%5D=Senatsverwaltung+f√ºr+Kultur+und+Gesellschaftlichen+Zusammenhalt&institutions%5B%5D=Senatsverwaltung+f√ºr+Stadtentwicklung%2C+Bauen+und+Wohnen&institutions%5B%5D=Senatsverwaltung+f√ºr+Mobilit√§t%2C+Verkehr%2C+Klimaschutz+und+Umwelt&institutions%5B%5D=Senatsverwaltung+f√ºr+Wirtschaft%2C+Energie+und+Betriebe&institutions%5B%5D=Senatsverwaltung+f√ºr+Wissenschaft%2C+Gesundheit+und+Pflege&alle-bezirksamt=on&institutions%5B%5D=Bezirksamt+Charlottenburg-Wilmersdorf&institutions%5B%5D=Bezirksamt+Friedrichshain-Kreuzberg&institutions%5B%5D=Bezirksamt+Lichtenberg&institutions%5B%5D=Bezirksamt+Marzahn-Hellersdorf&institutions%5B%5D=Bezirksamt+Mitte&institutions%5B%5D=Bezirksamt+Neuk√∂lln&institutions%5B%5D=Bezirksamt+Pankow&institutions%5B%5D=Bezirksamt+Reinickendorf&institutions%5B%5D=Bezirksamt+Spandau&institutions%5B%5D=Bezirksamt+Steglitz-Zehlendorf&institutions%5B%5D=Bezirksamt+Tempelhof-Sch√∂neberg&institutions%5B%5D=Bezirksamt+Treptow-K√∂penick&alle-landesbeauftragte=on&institutions%5B%5D=Beauftragte+des+Senats+f√ºr+Integration+und+Migration&institutions%5B%5D=Beauftragter+zur+Aufarbeitung+der+SED-Diktatur&institutions%5B%5D=B√ºrger-+und+Polizeibeauftragter+des+Landes+Berlin&institutions%5B%5D=Pflegebeauftragte+des+Landes+Berlin&institutions%5B%5D=Landestierschutzbeauftragte&institutions%5B%5D=Landeswahlleitung&bt=) und so alle Pressemitteilungen dieser Abteilungen abrufen: <!-- After that we can [perform search with selected depatrmentes and an empty query](https://www.berlin.de/presse/pressemitteilungen/index/search/?searchtext=&boolean=0&startdate=&enddate=&alle-senatsverwaltungen=on&institutions%5B%5D=Presse-+und+Informationsamt+des+Landes+Berlin&institutions%5B%5D=Senatsverwaltung+f√ºr+Bildung%2C+Jugend+und+Familie&institutions%5B%5D=Senatsverwaltung+f√ºr+Finanzen&institutions%5B%5D=Senatsverwaltung+f√ºr+Inneres+und+Sport&institutions%5B%5D=Senatsverwaltung+f√ºr+Arbeit%2C+Soziales%2C+Gleichstellung%2C+Integration%2C+Vielfalt+und+Antidiskriminierung&institutions%5B%5D=Senatsverwaltung+f√ºr+Justiz+und+Verbraucherschutz&institutions%5B%5D=Senatsverwaltung+f√ºr+Kultur+und+Gesellschaftlichen+Zusammenhalt&institutions%5B%5D=Senatsverwaltung+f√ºr+Stadtentwicklung%2C+Bauen+und+Wohnen&institutions%5B%5D=Senatsverwaltung+f√ºr+Mobilit√§t%2C+Verkehr%2C+Klimaschutz+und+Umwelt&institutions%5B%5D=Senatsverwaltung+f√ºr+Wirtschaft%2C+Energie+und+Betriebe&institutions%5B%5D=Senatsverwaltung+f√ºr+Wissenschaft%2C+Gesundheit+und+Pflege&alle-bezirksamt=on&institutions%5B%5D=Bezirksamt+Charlottenburg-Wilmersdorf&institutions%5B%5D=Bezirksamt+Friedrichshain-Kreuzberg&institutions%5B%5D=Bezirksamt+Lichtenberg&institutions%5B%5D=Bezirksamt+Marzahn-Hellersdorf&institutions%5B%5D=Bezirksamt+Mitte&institutions%5B%5D=Bezirksamt+Neuk√∂lln&institutions%5B%5D=Bezirksamt+Pankow&institutions%5B%5D=Bezirksamt+Reinickendorf&institutions%5B%5D=Bezirksamt+Spandau&institutions%5B%5D=Bezirksamt+Steglitz-Zehlendorf&institutions%5B%5D=Bezirksamt+Tempelhof-Sch√∂neberg&institutions%5B%5D=Bezirksamt+Treptow-K√∂penick&alle-landesbeauftragte=on&institutions%5B%5D=Beauftragte+des+Senats+f√ºr+Integration+und+Migration&institutions%5B%5D=Beauftragter+zur+Aufarbeitung+der+SED-Diktatur&institutions%5B%5D=B√ºrger-+und+Polizeibeauftragter+des+Landes+Berlin&institutions%5B%5D=Pflegebeauftragte+des+Landes+Berlin&institutions%5B%5D=Landestierschutzbeauftragte&institutions%5B%5D=Landeswahlleitung&bt=) and retrieve all press releases belonging to these departments:-->\n",
    "\n",
    "![suchergebnisse](../book_images/suchergebnisse.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd435155-4cbf-456d-b39e-955b99b362e8",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "Wir sehen, dass die Links hier in einer Tabelle gespeichert sind. In HTML wird eine Tabelle mit dem `<table>`-Element dargestellt. Wenn wir den Quellcode dieser Seite betrachten, stellen wir fest, dass sie eine Tabelle enth√§lt, in der alle Links aufgef√ºhrt sind:\n",
    "\n",
    "![selection](../book_images/pm_table_source_html.png) \n",
    "\n",
    "Um diese Links zu durchsuchen, k√∂nnen wir die grundlegenden HTML-Abfragefunktionen der bereits bekannten Bibliothek BeautifulSoup verwenden. Das machen wir im n√§chsten Abschnitt."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd62da74-4319-486d-adee-9058dbc9f495",
   "metadata": {},
   "source": [
    "## Suchergebnisse scrapen und Pressemitteilungen extrahieren (auf einer Seite): "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f973641b-67cf-4a7c-8f28-a51b92e03e07",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# -- organise data ----------------------------------------------------\n",
    "FIRST_OUTPUT_PAGE = (\n",
    "    \"https://www.berlin.de/presse/pressemitteilungen/index/search/?searchtext=&boolean=0&startdate=&enddate=&alle-senatsverwaltungen=on&institutions%5B%5D=Presse-+und+Informationsamt+des+Landes+Berlin&institutions%5B%5D=Senatsverwaltung+f√ºr+Bildung%2C+Jugend+und+Familie&institutions%5B%5D=Senatsverwaltung+f√ºr+Finanzen&institutions%5B%5D=Senatsverwaltung+f√ºr+Inneres+und+Sport&institutions%5B%5D=Senatsverwaltung+f√ºr+Arbeit%2C+Soziales%2C+Gleichstellung%2C+Integration%2C+Vielfalt+und+Antidiskriminierung&institutions%5B%5D=Senatsverwaltung+f√ºr+Justiz+und+Verbraucherschutz&institutions%5B%5D=Senatsverwaltung+f√ºr+Kultur+und+Gesellschaftlichen+Zusammenhalt&institutions%5B%5D=Senatsverwaltung+f√ºr+Stadtentwicklung%2C+Bauen+und+Wohnen&institutions%5B%5D=Senatsverwaltung+f√ºr+Mobilit√§t%2C+Verkehr%2C+Klimaschutz+und+Umwelt&institutions%5B%5D=Senatsverwaltung+f√ºr+Wirtschaft%2C+Energie+und+Betriebe&institutions%5B%5D=Senatsverwaltung+f√ºr+Wissenschaft%2C+Gesundheit+und+Pflege&alle-bezirksamt=on&institutions%5B%5D=Bezirksamt+Charlottenburg-Wilmersdorf&institutions%5B%5D=Bezirksamt+Friedrichshain-Kreuzberg&institutions%5B%5D=Bezirksamt+Lichtenberg&institutions%5B%5D=Bezirksamt+Marzahn-Hellersdorf&institutions%5B%5D=Bezirksamt+Mitte&institutions%5B%5D=Bezirksamt+Neuk√∂lln&institutions%5B%5D=Bezirksamt+Pankow&institutions%5B%5D=Bezirksamt+Reinickendorf&institutions%5B%5D=Bezirksamt+Spandau&institutions%5B%5D=Bezirksamt+Steglitz-Zehlendorf&institutions%5B%5D=Bezirksamt+Tempelhof-Sch√∂neberg&institutions%5B%5D=Bezirksamt+Treptow-K√∂penick&alle-landesbeauftragte=on&institutions%5B%5D=Beauftragte+des+Senats+f√ºr+Integration+und+Migration&institutions%5B%5D=Beauftragter+zur+Aufarbeitung+der+SED-Diktatur&institutions%5B%5D=B√ºrger-+und+Polizeibeauftragter+des+Landes+Berlin&institutions%5B%5D=Pflegebeauftragte+des+Landes+Berlin&institutions%5B%5D=Landestierschutzbeauftragte&institutions%5B%5D=Landeswahlleitung&bt=\"\n",
    ") \n",
    "DATA_DIR   = pathlib.Path(\"../data\")         \n",
    "HTML_DIR   = DATA_DIR / \"html\"\n",
    "TXT_DIR    = DATA_DIR / \"txt\"\n",
    "HTML_DIR.mkdir(parents=True, exist_ok=True)\n",
    "TXT_DIR.mkdir(parents=True,  exist_ok=True)\n",
    "logging.basicConfig(level=logging.INFO, format=\"%(levelname)s: %(message)s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d83d1d3-86af-41db-a8db-b14b270d17bb",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# -- helper -----------------------------------------------------------------\n",
    "\n",
    "def get_soup(url: str) -> BeautifulSoup:\n",
    "    \"\"\"Download a page and return BeautifulSoup (retry politely on transient errors).\"\"\"\n",
    "    while True:\n",
    "        r = requests.get(\n",
    "            url,\n",
    "            timeout=20,\n",
    "            headers={\"User-Agent\": \"Mozilla/5.0 (compatible; QuadrigaScraper/1.0)\"}\n",
    "        )\n",
    "        if r.status_code == 200:\n",
    "            return BeautifulSoup(r.text, \"lxml\")\n",
    "        logging.warning(\"Status %s on %s ‚Äì retrying in 5 s\", r.status_code, url)\n",
    "        time.sleep(5)\n",
    "\n",
    "def slugify(text_: str, maxlen: int = 60) -> str:\n",
    "    \"\"\"Rough filename-safe slug for headlines.\"\"\"\n",
    "    text_ = re.sub(r\"\\W+\", \"-\", text_.lower()).strip(\"-\")\n",
    "    return text_[:maxlen] or \"untitled\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a49834fb-f7c2-4811-8319-369c5cbc6bb9",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "hide-cell"
    ]
   },
   "source": [
    "<!-- <button onclick=\"toggleDisplay('myDIV1')\">Was passiert hier oben? (Schritt-f√ºr-Schritt-Erkl√§rung)</button>\n",
    "\n",
    "<div id=\"myDIV1\" style=\"display:none\"> -->\n",
    "    \n",
    "### Was passiert in diesem Codeblock oben?\n",
    "\n",
    "1. **`get_soup()`**\n",
    "\n",
    "   * Die Funktion versucht, eine Webseite herunterzuladen und sofort als `BeautifulSoup`-Objekt zur√ºckzugeben.\n",
    "   * Sie sendet einen HTTP-Request mit\n",
    "\n",
    "     * 20 Sekunden Timeout (sch√ºtzt vor ewig h√§ngenden Verbindungen) und\n",
    "     * einem eigenen *User-Agent*-Header, damit der Server wei√ü, dass es sich um ein automatisiertes, aber h√∂fliches Skript handelt (‚ÄûQuadrigaScraper/1.0‚Äú).\n",
    "   * Wenn der Server **Status 200** liefert ‚Üí der HTML-Text wird geparst und zur√ºckgegeben.\n",
    "   * Bei jedem anderen Statuscode wird eine Warnung ins Log geschrieben, 5 Sekunden gewartet und dann erneut versucht. Dadurch bricht der Scraper nicht sofort ab, sondern behandelt tempor√§re Fehler (z. B. 500er oder Netzwerk-Glitches) selbstst√§ndig.\n",
    "\n",
    "2. **`slugify()`**\n",
    "\n",
    "   * Wandelt eine √úberschrift (oder beliebigen Text) in einen dateisystemtauglichen ‚ÄûSlug‚Äú um.\n",
    "   * Schritte:\n",
    "\n",
    "     1. alles in Kleinbuchstaben verwandeln,\n",
    "     2. alle Zeichen, die **nicht** Buchstaben, Ziffern oder Unterstrich sind, durch Bindestriche ersetzen (`\\W+`),\n",
    "     3. f√ºhrende/abschlie√üende Bindestriche entfernen,\n",
    "     4. das Ergebnis auf maximal 60 Zeichen k√ºrzen (damit Dateinamen handlich bleiben).\n",
    "   * Falls nach allen Filterungen nichts √ºbrig ist, liefert die Funktion sicherheitshalber den Platzhalter **‚Äûuntitled‚Äú** zur√ºck.\n",
    "\n",
    "<!-- </div>\n",
    "\n",
    "<script>\n",
    "\n",
    "function toggleDisplay(id) {\n",
    "  var x = document.getElementById(id);\n",
    "  if (x.style.display === \"none\") {\n",
    "    x.style.display = \"block\";\n",
    "  } else {\n",
    "    x.style.display = \"none\";\n",
    "  }\n",
    "}\n",
    "</script> -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "577fdf92-4f28-4a8a-8dfa-5c607098d6a9",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "skip-execution"
    ]
   },
   "outputs": [],
   "source": [
    "# -- step 1: parse ONE results page -----------------------------------------\n",
    "search_soup = get_soup(FIRST_OUTPUT_PAGE)\n",
    "\n",
    "rows = search_soup.select(\"table tbody tr\")  \n",
    "records = []\n",
    "print(f\"Found {len(rows)} rows on the page.\")\n",
    "\n",
    "for tr in tqdm(rows, desc=\"Rows\"):\n",
    "    # grab all data cells once\n",
    "    cells = tr.find_all(\"td\")\n",
    "    if len(cells) < 3:          # footer / empty rows ‚Üí ignore\n",
    "        continue\n",
    "\n",
    "    # column 1 ‚Äì date\n",
    "    date_txt = cells[0].get_text(strip=True)\n",
    "\n",
    "    # column 2 ‚Äì headline + link\n",
    "    anchor = cells[1].find(\"a\", href=True)\n",
    "    if anchor is None:          # safety check\n",
    "        continue\n",
    "    title   = anchor.get_text(strip=True)\n",
    "    pr_url  = \"https://www.berlin.de\" + anchor[\"href\"]\n",
    "\n",
    "    # column 3 ‚Äì issuing authority (‚ÄúRessort‚Äù)\n",
    "    ressort = cells[2].get_text(strip=True)\n",
    "\n",
    "    # deterministic ID, e.g. 1570469\n",
    "    uid = anchor[\"href\"].split(\"/\")[-1].split(\".\")[-2]\n",
    "\n",
    "        # -- step 2: download the press release itself -----------\n",
    "    html_file = HTML_DIR / f\"{uid}.html\"\n",
    "    txt_file  = TXT_DIR  / f\"{uid}.txt\"\n",
    "\n",
    "    if not html_file.exists():        # skip if already scraped\n",
    "        pr_soup = get_soup(pr_url)\n",
    "\n",
    "        # write raw HTML\n",
    "        html_file.write_text(str(pr_soup), encoding=\"utf-8\")\n",
    "\n",
    "        # extract main text; fallback to whole page if CSS id changes\n",
    "        body = (pr_soup.select_one(\"#article\") or        # new layout (2024)\n",
    "                pr_soup.select_one(\"#content\") or        # classic layout\n",
    "                pr_soup)                                 # last resort\n",
    "        clean_text = body.get_text(\" \", strip=True)\n",
    "        txt_file.write_text(clean_text, encoding=\"utf-8\")\n",
    "    else:\n",
    "        # we still need the plain text length for the DataFrame below\n",
    "        clean_text = txt_file.read_text(encoding=\"utf-8\")\n",
    "\n",
    "    records.append(\n",
    "        dict(\n",
    "            date=date_txt,\n",
    "            ressort=ressort,\n",
    "            title=title,\n",
    "            pr_url=pr_url,\n",
    "            filename_html=html_file.name,\n",
    "            filename_txt=txt_file.name,\n",
    "            n_tokens=len(clean_text.split())\n",
    "        )\n",
    "    )\n",
    "    time.sleep(0.4)      # politeness\n",
    "\n",
    "# -- step 3: inspect the harvested metadata ---------------------------------\n",
    "df = pd.DataFrame(records)\n",
    "df.head()          # normal Jupyter display is fine ‚Äï no extra libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cba200c-4ff3-4606-8ad1-06814b431af2",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "hide-cell"
    ]
   },
   "source": [
    "<!-- <button onclick=\"toggleDisplay('myDIV2')\">Was passiert hier oben? (Schritt-f√ºr-Schritt-Erkl√§rung)</button>\n",
    "\n",
    "<div id=\"myDIV2\" style=\"display:none\"> -->\n",
    "\n",
    "### Was passiert in diesem Codeblock oben? ‚Äì Schritt f√ºr Schritt\n",
    "\n",
    "1. **Erste Seite einlesen**\n",
    "\n",
    "   ```python\n",
    "   search_soup = get_soup(SAMPLE_OUTPUT_PAGE)\n",
    "   rows = search_soup.select(\"table tbody tr\")\n",
    "   ```\n",
    "\n",
    "   * Die Funktion `get_soup()` l√§dt genau **eine** Ergebnisseite der\n",
    "     Such-/Listenansicht und gibt sie als Beautiful-Soup-Objekt zur√ºck.\n",
    "   * Mit dem CSS-Selektor `table tbody tr` werden alle Tabellenzeilen\n",
    "     der Ergebnisliste eingesammelt. Jede Zeile repr√§sentiert eine\n",
    "     einzelne Pressemitteilung.\n",
    "\n",
    "2. **Vorbereitung f√ºr die Schleife**\n",
    "\n",
    "   ```python\n",
    "   records = []\n",
    "   print(f\"Found {len(rows)} rows on the page.\")\n",
    "   ```\n",
    "\n",
    "   * `records` soll sp√§ter eine Liste von Dictionaries f√ºr das\n",
    "     DataFrame sammeln.\n",
    "   * Eine kurze Ausgabe zeigt, wie viele Zeilen tats√§chlich gefunden\n",
    "     wurden ‚Äì n√ºtzlich f√ºr Kontroll-/Debug-Zwecke.\n",
    "\n",
    "3. **Iterieren mit Fortschrittsbalken**\n",
    "\n",
    "   ```python\n",
    "   for tr in tqdm(rows, desc=\"Rows\"):\n",
    "   ```\n",
    "\n",
    "   * `tqdm` liefert einen h√ºbschen Fortschrittsbalken ‚Äì perfekt f√ºr\n",
    "     Lehr- und Live-Demos.\n",
    "\n",
    "4. **Zellen extrahieren & Plausibilit√§t pr√ºfen**\n",
    "\n",
    "   ```python\n",
    "   cells = tr.find_all(\"td\")\n",
    "   if len(cells) < 3:          # footer / empty rows ‚Üí ignore\n",
    "       continue\n",
    "   ```\n",
    "\n",
    "   * Alle `<td>` einer Zeile werden auf einmal geholt.\n",
    "   * Hat eine Zeile weniger als drei Zellen, handelt es sich um\n",
    "     Paginierungs- oder Leerzeilen; die werden √ºbersprungen.\n",
    "\n",
    "5. **Spalte 1 ‚Äì Datum**\n",
    "\n",
    "   ```python\n",
    "   date_txt = cells[0].get_text(strip=True)\n",
    "   ```\n",
    "\n",
    "   * `strip=True` entfernt Zeilenumbr√ºche und Leerzeichen ‚Äì wir erhalten\n",
    "     saubere Strings wie ‚Äû16.06.2025‚Äú.\n",
    "\n",
    "6. **Spalte 2 ‚Äì √úberschrift & Link**\n",
    "\n",
    "   ```python\n",
    "   anchor = cells[1].find(\"a\", href=True)\n",
    "   if anchor is None:\n",
    "       continue\n",
    "   title  = anchor.get_text(strip=True)\n",
    "   pr_url = \"https://www.berlin.de\" + anchor[\"href\"]\n",
    "   ```\n",
    "\n",
    "   * Innerhalb der zweiten Zelle steckt der anklickbare Link.\n",
    "   * Sicherheits-Check: Falls doch kein `<a>` vorhanden ist, Zeile\n",
    "     √ºberspringen.\n",
    "   * Die relative URL wird zur vollst√§ndigen URL erg√§nzt.\n",
    "\n",
    "7. **Spalte 3 ‚Äì Ressort (herausgebende Beh√∂rde)**\n",
    "\n",
    "   ```python\n",
    "   ressort = cells[2].get_text(strip=True)\n",
    "   ```\n",
    "\n",
    "8. **Eindeutige ID ableiten**\n",
    "\n",
    "   ```python\n",
    "   uid = anchor[\"href\"].split(\"/\")[-1].split(\".\")[-2]\n",
    "   ```\n",
    "\n",
    "   * Vom Pfadsegment `pressemitteilung.1570469.php` wird mittels\n",
    "     `split()` das numerische St√ºck **1570469** herausgel√∂st.\n",
    "   * Diese ID landet sp√§ter im Dateinamen, damit jeder Release genau\n",
    "     eine HTML- und eine TXT-Datei bekommt.\n",
    "\n",
    "9. **Dateipfade festlegen**\n",
    "\n",
    "   ```python\n",
    "   html_file = HTML_DIR / f\"{uid}.html\"\n",
    "   txt_file  = TXT_DIR  / f\"{uid}.txt\"\n",
    "   ```\n",
    "\n",
    "10. **HTML herunterladen & Text extrahieren (nur falls neu)**\n",
    "\n",
    "    ```python\n",
    "    if not html_file.exists():\n",
    "        pr_soup = get_soup(pr_url)\n",
    "        html_file.write_text(str(pr_soup), encoding=\"utf-8\")\n",
    "\n",
    "        body = (pr_soup.select_one(\"#article\")     # neues Layout\n",
    "                or pr_soup.select_one(\"#content\")  # altes Layout\n",
    "                or pr_soup)                        # Fallback\n",
    "        clean_text = body.get_text(\" \", strip=True)\n",
    "        txt_file.write_text(clean_text, encoding=\"utf-8\")\n",
    "    else:\n",
    "        clean_text = txt_file.read_text(encoding=\"utf-8\")\n",
    "    ```\n",
    "\n",
    "    * **Idempotenz**: Wenn die Datei schon existiert, wird nichts\n",
    "      erneut heruntergeladen ‚Äì das spart Zeit und Traffic.\n",
    "    * Der eigentliche Text sitzt mal in `#article`, mal in\n",
    "      `#content`. Wir probieren beide Selektoren und greifen im Zweifel\n",
    "      auf die ganze Seite zur√ºck.\n",
    "    * HTML und gereinigter Plain-Text werden getrennt gespeichert.\n",
    "\n",
    "11. **Metadaten sammeln**\n",
    "\n",
    "    ```python\n",
    "    records.append(\n",
    "        dict(\n",
    "            date=date_txt,\n",
    "            ressort=ressort,\n",
    "            title=title,\n",
    "            pr_url=pr_url,\n",
    "            filename_html=html_file.name,\n",
    "            filename_txt=txt_file.name,\n",
    "            n_tokens=len(clean_text.split())\n",
    "        )\n",
    "    )\n",
    "    ```\n",
    "\n",
    "    * Alle wesentlichen Infos ‚Äì inklusive Dateinamen und Token-Anzahl ‚Äì\n",
    "      landen in einem Dictionary, das wir sp√§ter direkt in ein\n",
    "      DataFrame gie√üen.\n",
    "\n",
    "12. **H√∂fliche Pause**\n",
    "\n",
    "    ```python\n",
    "    time.sleep(0.4)\n",
    "    ```\n",
    "\n",
    "    * 400 ms warten verringert die Gefahr, den Server zu √ºberlasten.\n",
    "\n",
    "13. **Auswertung in Pandas**\n",
    "\n",
    "    ```python\n",
    "    df = pd.DataFrame(records)\n",
    "    df.head()\n",
    "    ```\n",
    "\n",
    "    * Am Ende verwandeln wir die gesammelten Dictionaries in ein\n",
    "      `DataFrame`, um die ersten Zeilen gleich im Notebook\n",
    "      inspizieren zu k√∂nnen.\n",
    "\n",
    "So wird auf anschauliche Weise demonstriert, wie man **gezielt Teile einer\n",
    "HTML-Tabelle parst**, die Detailseiten herunterl√§dt, Text extrahiert und alles\n",
    "sauber f√ºr weitere Analysen ablegt.\n",
    "\n",
    "<!-- </div> -->\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b86ad42-b2d0-49a5-9c83-f2bb151f5f01",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## Massenscraping von Pressemitteilungen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d489611-65f5-4542-96ff-e1d2b41fa71d",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ‚ïî‚ïê‚ï° 80_pagination_helpers\n",
    "# Basis-URL ohne \"/page/<nr>\"-Segment,\n",
    "# aber *inklusive* aller Query-Parameter (= Filter der langen Such-URL)\n",
    "\n",
    "SEARCH_ROOT = FIRST_OUTPUT_PAGE\n",
    "\n",
    "def last_page_number() -> int:\n",
    "    \"\"\"\n",
    "    Ermittelt √ºber das <nav>-Element ('pager-skip-to-last') die h√∂chste\n",
    "    Ergebnisseiten-Nummer.\n",
    "    \"\"\"\n",
    "    soup = get_soup(SEARCH_ROOT)                            # 1. Suchseite laden\n",
    "    last_link = soup.select_one(\"li.pager-skip-to-last a\")\n",
    "    if not last_link:\n",
    "        raise RuntimeError(\"Konnte die letzte Seite nicht finden ‚Äì Selector?\")\n",
    "    \n",
    "    # href hat die Form \".../page/5239?<query>\"\n",
    "    m = re.search(r\"/page/(\\d+)\", last_link[\"href\"])\n",
    "    if not m:\n",
    "        raise RuntimeError(\"Seitenzahl nicht im href gefunden.\")\n",
    "    return int(m.group(1))\n",
    "\n",
    "def page_url(page_num: int) -> str:\n",
    "    \"\"\"\n",
    "    Baut die URL f√ºr eine beliebige Seite nach folgendem Muster auf:\n",
    "    <root>/page/<nr>?<identische Query-Parameter>\n",
    "    \"\"\"\n",
    "    if page_num < 1:\n",
    "        raise ValueError(\"Seitennummern beginnen bei 1.\")\n",
    "    return SEARCH_ROOT.replace(\"/search/\", f\"/search/page/{page_num}/\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76d94122-daf5-4ef4-9790-e44122ce6e1f",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ‚ïî‚ïê‚ï° 81_bulk_crawler\n",
    "def crawl_all_pages(pages: int | None = None, sleep_s: float = 0.4) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    L√§uft √ºber alle Treffer-Seiten, sammelt und l√§dt jede Pressemitteilung.\n",
    "    Gibt ein DataFrame mit s√§mtlichen Metadaten zur√ºck.\n",
    "    \n",
    "    *pages*  ‚Äì Anzahl der Seiten; None ‚Üí automatisch ermitteln  \n",
    "    *sleep_s*‚Äì H√∂flichkeits-Delay zwischen Requests\n",
    "    \"\"\"\n",
    "    if pages is None:\n",
    "        pages = last_page_number()\n",
    "        print(f\"‚Üí Letzte Trefferseite lautet {pages}\")\n",
    "\n",
    "    all_records: list[dict] = []\n",
    "    for p in tqdm(range(1, pages + 1), desc=\"Result pages\"):\n",
    "        soup = get_soup(page_url(p))\n",
    "        rows = soup.select(\"table tbody tr\")\n",
    "        \n",
    "        for tr in rows:\n",
    "            cells = tr.find_all(\"td\")\n",
    "            if len(cells) < 3:\n",
    "                continue  # Skip Leer-/Footer-Zeilen\n",
    "\n",
    "            date_txt = cells[0].get_text(strip=True)\n",
    "            anchor   = cells[1].find(\"a\", href=True)\n",
    "            if anchor is None:\n",
    "                continue\n",
    "            title    = anchor.get_text(strip=True)\n",
    "            pr_url   = \"https://www.berlin.de\" + anchor[\"href\"]\n",
    "            ressort  = cells[2].get_text(strip=True)\n",
    "            uid      = anchor[\"href\"].split(\".\")[-2]  # z. B. 1570469\n",
    "            html_fp  = HTML_DIR / f\"{uid}.html\"\n",
    "            txt_fp   = TXT_DIR  / f\"{uid}.txt\"\n",
    "\n",
    "            # ----- Detailseite nur laden, wenn noch nicht vorhanden -----------\n",
    "            if not html_fp.exists():\n",
    "                pr_soup = get_soup(pr_url)\n",
    "                html_fp.write_text(str(pr_soup), encoding=\"utf-8\")\n",
    "                body = (pr_soup.select_one(\"#article\") or\n",
    "                        pr_soup.select_one(\"#content\") or\n",
    "                        pr_soup)\n",
    "                clean = body.get_text(\" \", strip=True)\n",
    "                txt_fp.write_text(clean, encoding=\"utf-8\")\n",
    "            else:\n",
    "                clean = txt_fp.read_text(encoding=\"utf-8\")\n",
    "\n",
    "            all_records.append(\n",
    "                dict(\n",
    "                    date=date_txt,\n",
    "                    ressort=ressort,\n",
    "                    title=title,\n",
    "                    pr_url=pr_url,\n",
    "                    filename_html=html_fp.name,\n",
    "                    filename_txt=txt_fp.name,\n",
    "                    n_tokens=len(clean.split())\n",
    "                )\n",
    "            )\n",
    "            time.sleep(sleep_s)   # H√∂flichkeit: Delay pro Detail-Request\n",
    "\n",
    "    return pd.DataFrame(all_records)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8472519c-13bc-454e-9e8a-06a727816374",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ‚ïî‚ïê‚ï° 82_test_run_5_pages\n",
    "df_test = crawl_all_pages(pages=5)\n",
    "df_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "752fee21-6192-47a2-812b-ebc10e814bb8",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "skip-execution"
    ]
   },
   "outputs": [],
   "source": [
    "# ‚ïî‚ïê‚ï° 83_run_bulk\n",
    "# Achtung: Das kann > 1 Stunde dauern und tausende Dateien erzeugen!\n",
    "df_all = crawl_all_pages(pages=None)   # None ‚Üí auto-detect\n",
    "df_all.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a8a3d76-7955-4e5a-9749-3c6d8ff42bbe",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "### Export von Metadaten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb429b69-1a01-4a91-8764-f0e32839b234",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "skip-execution"
    ]
   },
   "outputs": [],
   "source": [
    "metadata_path = DATA_DIR / \"metadata_updated.csv\"\n",
    "df_all.to_csv()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
