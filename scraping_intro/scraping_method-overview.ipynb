{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7b94d2ff",
   "metadata": {},
   "source": [
    "# üöÄ Web-Scraping als Methode der Korpuserstellung"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dc3842c",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\"> <b> üîî Feinlernziel(e) dieses Kapitels</b></br>\n",
    "Sie k√∂nnen verschiedene Methoden der Website-Abfrage aufz√§hlen und Unterschiede identifizieren. </br>\n",
    "Sie k√∂nnen Vor- und Nachteile der Methoden erkl√§ren und ermitteln, in welchen Szenarien welche Methode geeignet ist.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68ef46b6",
   "metadata": {},
   "source": [
    "## Hinweise zur Ausf√ºhrung des Notebooks\n",
    "Dieses Notebook kann auf unterschiedlichen Levels erarbeitet werden (siehe Abschnitt [\"Technische Voraussetzungen\"](../introduction/introduction_requirements)): \n",
    "1. Book-Only Mode\n",
    "2. Cloud Mode: Daf√ºr auf üöÄ klicken und z.B. in Colab ausf√ºhren.\n",
    "3. Local Mode: Daf√ºr auf Herunterladen ‚Üì klicken und \".ipynb\" w√§hlen. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2a5e988-19ab-44be-971e-d9d81fc6d7dd",
   "metadata": {},
   "source": [
    "## Hinweise zur Ausf√ºhrung des Notebooks\n",
    "Dieses Notebook kann auf unterschiedlichen Levels erarbeitet werden (siehe Abschnitt [\"Technische Voraussetzungen\"](../markdown/introduction_requirements)): \n",
    "1. Book-Only Mode\n",
    "2. Cloud Mode: Daf√ºr auf üöÄ klicken und z.B. in Colab ausf√ºhren.\n",
    "3. Local Mode: Daf√ºr auf Herunterladen ‚Üì klicken und \".ipynb\" w√§hlen. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86edc36a",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "<details>\n",
    "  <summary><b>Informationen zum Ausf√ºhren des Notebooks ‚Äì Zum Ausklappen klicken ‚¨áÔ∏è</b></summary>\n",
    "  \n",
    "<b>Voraussetzungen zur Ausf√ºhrung des Jupyter Notebooks:</b>\n",
    "<ul>\n",
    "<li> Installieren der Bibliotheken </li>\n",
    "</ul>\n",
    "Zum Testen: Ausf√ºhren der Zelle \"load libraries\".</br>\n",
    "Alle Zellen, die mit üöÄ gekennzeichnet sind, werden nur bei der Ausf√ºhrung des Noteboos in Colab / JupyterHub bzw. lokal ausgef√ºhrt. \n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2b6c48e",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "skip-execution",
     "hide-output"
    ]
   },
   "outputs": [],
   "source": [
    "#  üöÄ Install libraries \n",
    "! pip install requests beautifulsoup4 scrapy selenium webdriver_manager time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "338177f1-96f7-415a-a7ef-b142cd10a395",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## Einf√ºhrung\n",
    "Im [vorherigen Kapitel](scraping-intro_http-intro) haben wir bereits ein Beispiel zur automatisierten Abfrage einer Website kennengelernt. Um mehr als eine Website abzufragen, gibt es verschiedene Methoden. Welche Methode sich am besten zur Extraktion eignet, h√§ngt davon ab, wie die abzufragenden Websites aufgebaut sind und ob sie rein statische oder auch dynamische Inhalte beinhalten. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6e1fe4d-d1ef-47b2-af97-6fe73164dca4",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "### Statische vs. dynamische Websites\n",
    "\n",
    "Websites k√∂nnen grunds√§tzlich in zwei Kategorien eingeteilt werden: statische und dynamische Websites. Abh√§ngig davon, welche Inhalte extrahiert werden sollen und wie die Website beschaffen ist, muss die Scraping-Methode angepasst werden.\n",
    "\n",
    "- **Statische Websites**: Diese Websites sind fertige Dokumente, die auf einem Server bereitliegen. Wenn Sie eine solche Website anfordern, wird Ihnen exakt dieser vorbereitete Inhalt geschickt. Das ist vergleichbar mit dem Anfordern eines bestimmten Buches aus einer Bibliothek ‚Äì der Inhalt liegt fertig vor und √§ndert sich nicht. Diese Art von Websites kann leicht mit einfachen Scraping-Methoden extrahiert werden, da alle Informationen direkt im HTML-Code enthalten sind.\n",
    "\n",
    "- **Dynamische Websites**: Diese Websites werden erst im Moment der Anfrage zusammengestellt. Sie enthalten oft JavaScript-Code, der nach dem Laden der Seite ausgef√ºhrt wird und weitere Inhalte nachladen oder ver√§ndern kann. Das ist vergleichbar mit einem Koch, der das Gericht erst auf Bestellung zubereitet. F√ºr die automatisierte Abfrage dieser Art von Websites ben√∂tigt man fortgeschrittenere Scraping-Methoden wie Selenium, die einen Browser simulieren k√∂nnen."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a833fd91-986e-4a3e-b797-bc3a0daa6919",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## Drei Ebenen des Web Scrapings\n",
    "\n",
    "### 1. Einfache Anfragen mit requests\n",
    "\n",
    "Die grundlegendste Form des Web Scrapings ist das Abrufen einzelner Webseiten, z.B. mit Hilfe des Python-Pakets `requests`. Diese Methode eignet sich f√ºr statische Webseiten, deren Inhalt direkt im HTML-Code enthalten ist."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "19f96d79-5adc-4fa4-bc35-f0c4198ba612",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Status-Code: 200\n",
      "\n",
      "Beginn des HTML-Dokuments:\n",
      "<!doctype html>\n",
      "<html xmlns=\"http://www.w3.org/1999/xhtml\" xml:lang=\"de\" lang=\"de\" data-lang=\"de\">\n",
      "<\n"
     ]
    }
   ],
   "source": [
    "# import library to perform HTTP requests\n",
    "import requests\n",
    "\n",
    "# Set URL \n",
    "url = \"https://www.berlin.de/rbmskzl/\"\n",
    "\n",
    "# perform get request\n",
    "response = requests.get(url)\n",
    "\n",
    "# check if request was successful (code: 200)\n",
    "if response.status_code == 200:\n",
    "    print(f\"Status-Code: {response.status_code}\")\n",
    "\n",
    "    # display the first lines of the response body (the content of the website)\n",
    "    print(\"\\nBeginn des HTML-Dokuments:\")\n",
    "    print(response.text[:100])\n",
    "else:\n",
    "    print(f\"Fehler beim Abrufen der Seite: {response.status_code}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdbd4cfc-3581-4dd3-a491-f6847fbe8767",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "**Vorteile:**\n",
    "- Einfach zu implementieren\n",
    "- Geringer Ressourcenverbrauch\n",
    "- Ausreichend f√ºr einfache Scraping-Aufgaben\n",
    "\n",
    "**Nachteile:**\n",
    "- Nur einzelne Seiten werden abgerufen\n",
    "- Keine automatische Navigation zu anderen Seiten\n",
    "- Nicht geeignet f√ºr dynamisch generierte Inhalte (JavaScript)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e491ca9-fbc2-455a-aef1-e96f7dc343b6",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "### 2. Navigation mit Scrapy\n",
    "\n",
    "F√ºr komplexere Scraping-Aufgaben, bei denen mehrere Seiten durchlaufen werden m√ºssen, eignet sich die Bibliothek `scrapy`. Sie erm√∂glicht das systematische Folgen von Links und das Extrahieren von Daten aus mehreren Seiten."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9078194d-65d5-4d69-ba40-f68bca63e27a",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "skip-execution",
     "hide-output"
    ]
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-05 16:58:21 [scrapy.utils.log] INFO: Scrapy 2.13.1 started (bot: scrapybot)\n",
      "2025-06-05 16:58:21 [scrapy.utils.log] INFO: Versions:\n",
      "{'lxml': '5.4.0',\n",
      " 'libxml2': '2.13.8',\n",
      " 'cssselect': '1.3.0',\n",
      " 'parsel': '1.10.0',\n",
      " 'w3lib': '2.3.1',\n",
      " 'Twisted': '24.11.0',\n",
      " 'Python': '3.12.8 (main, Dec  6 2024, 19:42:06) [Clang 18.1.8 ]',\n",
      " 'pyOpenSSL': '25.1.0 (OpenSSL 3.5.0 8 Apr 2025)',\n",
      " 'cryptography': '45.0.3',\n",
      " 'Platform': 'macOS-15.5-arm64-arm-64bit'}\n",
      "2025-06-05 16:58:21 [scrapy.addons] INFO: Enabled addons:\n",
      "[]\n",
      "2025-06-05 16:58:21 [scrapy.utils.log] DEBUG: Using reactor: twisted.internet.asyncioreactor.AsyncioSelectorReactor\n",
      "2025-06-05 16:58:21 [scrapy.utils.log] DEBUG: Using asyncio event loop: asyncio.unix_events._UnixSelectorEventLoop\n",
      "2025-06-05 16:58:21 [scrapy.extensions.telnet] INFO: Telnet Password: de83549a9333e40b\n",
      "2025-06-05 16:58:21 [scrapy.middleware] INFO: Enabled extensions:\n",
      "['scrapy.extensions.corestats.CoreStats',\n",
      " 'scrapy.extensions.telnet.TelnetConsole',\n",
      " 'scrapy.extensions.memusage.MemoryUsage',\n",
      " 'scrapy.extensions.feedexport.FeedExporter',\n",
      " 'scrapy.extensions.logstats.LogStats']\n",
      "2025-06-05 16:58:21 [scrapy.crawler] INFO: Overridden settings:\n",
      "{'USER_AGENT': 'mozilla/4.0 (compatible; msie 7.0; windows nt 5.1; gtb5)'}\n",
      "2025-06-05 16:58:21 [scrapy.middleware] INFO: Enabled downloader middlewares:\n",
      "['scrapy.downloadermiddlewares.offsite.OffsiteMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',\n",
      " 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',\n",
      " 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',\n",
      " 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',\n",
      " 'scrapy.downloadermiddlewares.retry.RetryMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',\n",
      " 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',\n",
      " 'scrapy.downloadermiddlewares.stats.DownloaderStats']\n",
      "2025-06-05 16:58:21 [scrapy.middleware] INFO: Enabled spider middlewares:\n",
      "['scrapy.spidermiddlewares.start.StartSpiderMiddleware',\n",
      " 'scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',\n",
      " 'scrapy.spidermiddlewares.referer.RefererMiddleware',\n",
      " 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',\n",
      " 'scrapy.spidermiddlewares.depth.DepthMiddleware']\n",
      "2025-06-05 16:58:21 [scrapy.middleware] INFO: Enabled item pipelines:\n",
      "[]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "This event loop is already running",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 30\u001b[0m\n\u001b[1;32m     22\u001b[0m process \u001b[38;5;241m=\u001b[39m CrawlerProcess({\n\u001b[1;32m     23\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mUSER_AGENT\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmozilla/4.0 (compatible; msie 7.0; windows nt 5.1; gtb5)\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m     24\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFEEDS\u001b[39m\u001b[38;5;124m'\u001b[39m: {\n\u001b[1;32m     25\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124moutput.json\u001b[39m\u001b[38;5;124m'\u001b[39m: {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mformat\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mjson\u001b[39m\u001b[38;5;124m'\u001b[39m},\n\u001b[1;32m     26\u001b[0m     }\n\u001b[1;32m     27\u001b[0m })\n\u001b[1;32m     29\u001b[0m process\u001b[38;5;241m.\u001b[39mcrawl(SenatSpider)\n\u001b[0;32m---> 30\u001b[0m \u001b[43mprocess\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstart\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# This will block until crawling is finished\u001b[39;00m\n",
      "File \u001b[0;32m~/Quadriga/Fallstudie_2/.venv/lib/python3.12/site-packages/scrapy/crawler.py:502\u001b[0m, in \u001b[0;36mCrawlerProcess.start\u001b[0;34m(self, stop_after_crawl, install_signal_handlers)\u001b[0m\n\u001b[1;32m    498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m install_signal_handlers:\n\u001b[1;32m    499\u001b[0m     reactor\u001b[38;5;241m.\u001b[39maddSystemEventTrigger(\n\u001b[1;32m    500\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mafter\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstartup\u001b[39m\u001b[38;5;124m\"\u001b[39m, install_shutdown_handlers, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_signal_shutdown\n\u001b[1;32m    501\u001b[0m     )\n\u001b[0;32m--> 502\u001b[0m \u001b[43mreactor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43minstallSignalHandlers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minstall_signal_handlers\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Quadriga/Fallstudie_2/.venv/lib/python3.12/site-packages/twisted/internet/asyncioreactor.py:253\u001b[0m, in \u001b[0;36mAsyncioSelectorReactor.run\u001b[0;34m(self, installSignalHandlers)\u001b[0m\n\u001b[1;32m    251\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrun\u001b[39m(\u001b[38;5;28mself\u001b[39m, installSignalHandlers\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[1;32m    252\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstartRunning(installSignalHandlers\u001b[38;5;241m=\u001b[39minstallSignalHandlers)\n\u001b[0;32m--> 253\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_asyncioEventloop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_forever\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    254\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_justStopped:\n\u001b[1;32m    255\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_justStopped \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/share/uv/python/cpython-3.12.8-macos-aarch64-none/lib/python3.12/asyncio/base_events.py:629\u001b[0m, in \u001b[0;36mBaseEventLoop.run_forever\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    627\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Run until stop() is called.\"\"\"\u001b[39;00m\n\u001b[1;32m    628\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_closed()\n\u001b[0;32m--> 629\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_check_running\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    630\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_set_coroutine_origin_tracking(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_debug)\n\u001b[1;32m    632\u001b[0m old_agen_hooks \u001b[38;5;241m=\u001b[39m sys\u001b[38;5;241m.\u001b[39mget_asyncgen_hooks()\n",
      "File \u001b[0;32m~/.local/share/uv/python/cpython-3.12.8-macos-aarch64-none/lib/python3.12/asyncio/base_events.py:621\u001b[0m, in \u001b[0;36mBaseEventLoop._check_running\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    619\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_check_running\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    620\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_running():\n\u001b[0;32m--> 621\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mThis event loop is already running\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    622\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m events\u001b[38;5;241m.\u001b[39m_get_running_loop() \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    623\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    624\u001b[0m             \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCannot run the event loop while another loop is running\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: This event loop is already running"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-05 16:58:21 [scrapy.core.engine] INFO: Spider opened\n",
      "2025-06-05 16:58:21 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)\n",
      "2025-06-05 16:58:21 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023\n",
      "2025-06-05 16:58:21 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.berlin.de/rbmskzl/> (referer: None)\n",
      "2025-06-05 16:58:21 [scrapy.core.scraper] DEBUG: Scraped from <200 https://www.berlin.de/rbmskzl/>\n",
      "{'title': None}\n",
      "2025-06-05 16:58:21 [scrapy.core.scraper] DEBUG: Scraped from <200 https://www.berlin.de/rbmskzl/>\n",
      "{'title': 'Olympia-Bewerbung'}\n",
      "2025-06-05 16:58:21 [scrapy.core.scraper] DEBUG: Scraped from <200 https://www.berlin.de/rbmskzl/>\n",
      "{'title': None}\n",
      "2025-06-05 16:58:21 [scrapy.core.scraper] DEBUG: Scraped from <200 https://www.berlin.de/rbmskzl/>\n",
      "{'title': None}\n",
      "2025-06-05 16:58:21 [scrapy.core.scraper] DEBUG: Scraped from <200 https://www.berlin.de/rbmskzl/>\n",
      "{'title': None}\n",
      "2025-06-05 16:58:21 [scrapy.core.scraper] DEBUG: Scraped from <200 https://www.berlin.de/rbmskzl/>\n",
      "{'title': None}\n",
      "2025-06-05 16:58:21 [scrapy.core.scraper] DEBUG: Scraped from <200 https://www.berlin.de/rbmskzl/>\n",
      "{'title': None}\n",
      "2025-06-05 16:58:21 [scrapy.core.scraper] DEBUG: Scraped from <200 https://www.berlin.de/rbmskzl/>\n",
      "{'title': None}\n",
      "2025-06-05 16:58:21 [scrapy.core.scraper] DEBUG: Scraped from <200 https://www.berlin.de/rbmskzl/>\n",
      "{'title': None}\n",
      "2025-06-05 16:58:21 [scrapy.core.scraper] DEBUG: Scraped from <200 https://www.berlin.de/rbmskzl/>\n",
      "{'title': None}\n",
      "2025-06-05 16:58:21 [scrapy.core.scraper] DEBUG: Scraped from <200 https://www.berlin.de/rbmskzl/>\n",
      "{'title': None}\n",
      "2025-06-05 16:58:21 [scrapy.core.scraper] DEBUG: Scraped from <200 https://www.berlin.de/rbmskzl/>\n",
      "{'title': None}\n",
      "2025-06-05 16:58:21 [scrapy.core.scraper] DEBUG: Scraped from <200 https://www.berlin.de/rbmskzl/>\n",
      "{'title': None}\n",
      "2025-06-05 16:58:21 [scrapy.core.scraper] DEBUG: Scraped from <200 https://www.berlin.de/rbmskzl/>\n",
      "{'title': None}\n",
      "2025-06-05 16:58:21 [scrapy.core.scraper] DEBUG: Scraped from <200 https://www.berlin.de/rbmskzl/>\n",
      "{'title': None}\n",
      "2025-06-05 16:58:21 [scrapy.core.scraper] DEBUG: Scraped from <200 https://www.berlin.de/rbmskzl/>\n",
      "{'title': None}\n",
      "2025-06-05 16:58:21 [scrapy.core.scraper] DEBUG: Scraped from <200 https://www.berlin.de/rbmskzl/>\n",
      "{'title': None}\n",
      "2025-06-05 16:58:21 [scrapy.core.scraper] DEBUG: Scraped from <200 https://www.berlin.de/rbmskzl/>\n",
      "{'title': None}\n",
      "2025-06-05 16:58:21 [scrapy.core.scraper] DEBUG: Scraped from <200 https://www.berlin.de/rbmskzl/>\n",
      "{'title': None}\n",
      "2025-06-05 16:58:21 [scrapy.core.scraper] DEBUG: Scraped from <200 https://www.berlin.de/rbmskzl/>\n",
      "{'title': None}\n",
      "2025-06-05 16:58:21 [scrapy.core.scraper] DEBUG: Scraped from <200 https://www.berlin.de/rbmskzl/>\n",
      "{'title': None}\n",
      "2025-06-05 16:58:21 [scrapy.core.scraper] DEBUG: Scraped from <200 https://www.berlin.de/rbmskzl/>\n",
      "{'title': None}\n",
      "2025-06-05 16:58:21 [scrapy.core.scraper] DEBUG: Scraped from <200 https://www.berlin.de/rbmskzl/>\n",
      "{'title': None}\n",
      "2025-06-05 16:58:21 [scrapy.core.scraper] DEBUG: Scraped from <200 https://www.berlin.de/rbmskzl/>\n",
      "{'title': None}\n",
      "2025-06-05 16:58:21 [scrapy.core.scraper] DEBUG: Scraped from <200 https://www.berlin.de/rbmskzl/>\n",
      "{'title': None}\n",
      "2025-06-05 16:58:21 [scrapy.core.scraper] DEBUG: Scraped from <200 https://www.berlin.de/rbmskzl/>\n",
      "{'title': None}\n",
      "2025-06-05 16:58:21 [scrapy.core.scraper] DEBUG: Scraped from <200 https://www.berlin.de/rbmskzl/>\n",
      "{'title': None}\n",
      "2025-06-05 16:58:21 [scrapy.core.scraper] DEBUG: Scraped from <200 https://www.berlin.de/rbmskzl/>\n",
      "{'title': None}\n",
      "2025-06-05 16:58:21 [scrapy.core.scraper] DEBUG: Scraped from <200 https://www.berlin.de/rbmskzl/>\n",
      "{'title': None}\n",
      "2025-06-05 16:58:21 [scrapy.core.scraper] DEBUG: Scraped from <200 https://www.berlin.de/rbmskzl/>\n",
      "{'title': None}\n",
      "2025-06-05 16:58:21 [scrapy.core.scraper] DEBUG: Scraped from <200 https://www.berlin.de/rbmskzl/>\n",
      "{'title': None}\n",
      "2025-06-05 16:58:21 [scrapy.core.scraper] DEBUG: Scraped from <200 https://www.berlin.de/rbmskzl/>\n",
      "{'title': 'Olympia-Bewerbung'}\n",
      "2025-06-05 16:58:21 [scrapy.core.scraper] DEBUG: Scraped from <200 https://www.berlin.de/rbmskzl/>\n",
      "{'title': None}\n",
      "2025-06-05 16:58:21 [scrapy.core.scraper] DEBUG: Scraped from <200 https://www.berlin.de/rbmskzl/>\n",
      "{'title': None}\n",
      "2025-06-05 16:58:21 [scrapy.core.scraper] DEBUG: Scraped from <200 https://www.berlin.de/rbmskzl/>\n",
      "{'title': 'Olympia-Bewerbung'}\n",
      "2025-06-05 16:58:21 [scrapy.core.scraper] DEBUG: Scraped from <200 https://www.berlin.de/rbmskzl/>\n",
      "{'title': 'Olympia-Bewerbung'}\n",
      "2025-06-05 16:58:21 [scrapy.core.scraper] DEBUG: Scraped from <200 https://www.berlin.de/rbmskzl/>\n",
      "{'title': 'Olympia-Bewerbung'}\n",
      "2025-06-05 16:58:21 [scrapy.core.scraper] DEBUG: Scraped from <200 https://www.berlin.de/rbmskzl/>\n",
      "{'title': 'Olympia-Bewerbung'}\n",
      "2025-06-05 16:58:21 [scrapy.core.scraper] DEBUG: Scraped from <200 https://www.berlin.de/rbmskzl/>\n",
      "{'title': None}\n",
      "2025-06-05 16:58:21 [scrapy.core.scraper] DEBUG: Scraped from <200 https://www.berlin.de/rbmskzl/>\n",
      "{'title': None}\n",
      "2025-06-05 16:58:21 [scrapy.core.scraper] DEBUG: Scraped from <200 https://www.berlin.de/rbmskzl/>\n",
      "{'title': None}\n",
      "2025-06-05 16:58:21 [scrapy.core.scraper] DEBUG: Scraped from <200 https://www.berlin.de/rbmskzl/>\n",
      "{'title': 'Olympia-Bewerbung'}\n",
      "2025-06-05 16:58:21 [scrapy.core.scraper] DEBUG: Scraped from <200 https://www.berlin.de/rbmskzl/>\n",
      "{'title': 'Olympia-Bewerbung'}\n",
      "2025-06-05 16:58:21 [scrapy.core.scraper] DEBUG: Scraped from <200 https://www.berlin.de/rbmskzl/>\n",
      "{'title': 'Ernennung von Sarah Wedl-Wilson zur Kultursenatorin'}\n",
      "2025-06-05 16:58:21 [scrapy.core.scraper] DEBUG: Scraped from <200 https://www.berlin.de/rbmskzl/>\n",
      "{'title': 'Senat vor Ort in Marzahn-Hellersdorf'}\n",
      "2025-06-05 16:58:21 [scrapy.core.scraper] DEBUG: Scraped from <200 https://www.berlin.de/rbmskzl/>\n",
      "{'title': 'Tag der offenen T√ºr im Roten Rathaus'}\n",
      "2025-06-05 16:58:21 [scrapy.core.scraper] DEBUG: Scraped from <200 https://www.berlin.de/rbmskzl/>\n",
      "{'title': 'Oft gesucht'}\n",
      "2025-06-05 16:58:21 [scrapy.core.scraper] DEBUG: Scraped from <200 https://www.berlin.de/rbmskzl/>\n",
      "{'title': 'Oft gesucht'}\n",
      "2025-06-05 16:58:21 [scrapy.core.scraper] DEBUG: Scraped from <200 https://www.berlin.de/rbmskzl/>\n",
      "{'title': None}\n",
      "2025-06-05 16:58:21 [scrapy.core.scraper] DEBUG: Scraped from <200 https://www.berlin.de/rbmskzl/>\n",
      "{'title': None}\n",
      "2025-06-05 16:58:21 [scrapy.core.scraper] DEBUG: Scraped from <200 https://www.berlin.de/rbmskzl/>\n",
      "{'title': None}\n",
      "2025-06-05 16:58:21 [scrapy.core.scraper] DEBUG: Scraped from <200 https://www.berlin.de/rbmskzl/>\n",
      "{'title': None}\n",
      "2025-06-05 16:58:21 [scrapy.core.scraper] DEBUG: Scraped from <200 https://www.berlin.de/rbmskzl/>\n",
      "{'title': None}\n",
      "2025-06-05 16:58:21 [scrapy.core.scraper] DEBUG: Scraped from <200 https://www.berlin.de/rbmskzl/>\n",
      "{'title': None}\n",
      "2025-06-05 16:58:21 [scrapy.core.scraper] DEBUG: Scraped from <200 https://www.berlin.de/rbmskzl/>\n",
      "{'title': None}\n",
      "2025-06-05 16:58:21 [scrapy.core.scraper] DEBUG: Scraped from <200 https://www.berlin.de/rbmskzl/>\n",
      "{'title': None}\n",
      "2025-06-05 16:58:21 [scrapy.core.scraper] DEBUG: Scraped from <200 https://www.berlin.de/rbmskzl/>\n",
      "{'title': None}\n",
      "2025-06-05 16:58:21 [scrapy.core.scraper] DEBUG: Scraped from <200 https://www.berlin.de/rbmskzl/>\n",
      "{'title': None}\n",
      "2025-06-05 16:58:21 [scrapy.core.scraper] DEBUG: Scraped from <200 https://www.berlin.de/rbmskzl/>\n",
      "{'title': None}\n",
      "2025-06-05 16:58:21 [scrapy.core.scraper] DEBUG: Scraped from <200 https://www.berlin.de/rbmskzl/>\n",
      "{'title': None}\n",
      "2025-06-05 16:58:21 [scrapy.core.scraper] DEBUG: Scraped from <200 https://www.berlin.de/rbmskzl/>\n",
      "{'title': None}\n",
      "2025-06-05 16:58:21 [scrapy.core.scraper] DEBUG: Scraped from <200 https://www.berlin.de/rbmskzl/>\n",
      "{'title': None}\n",
      "2025-06-05 16:58:21 [scrapy.core.scraper] DEBUG: Scraped from <200 https://www.berlin.de/rbmskzl/>\n",
      "{'title': None}\n",
      "2025-06-05 16:58:21 [scrapy.core.scraper] DEBUG: Scraped from <200 https://www.berlin.de/rbmskzl/>\n",
      "{'title': None}\n",
      "2025-06-05 16:58:21 [scrapy.core.scraper] DEBUG: Scraped from <200 https://www.berlin.de/rbmskzl/>\n",
      "{'title': None}\n",
      "2025-06-05 16:58:21 [scrapy.core.scraper] DEBUG: Scraped from <200 https://www.berlin.de/rbmskzl/>\n",
      "{'title': None}\n",
      "2025-06-05 16:58:21 [scrapy.core.scraper] DEBUG: Scraped from <200 https://www.berlin.de/rbmskzl/>\n",
      "{'title': None}\n",
      "2025-06-05 16:58:21 [scrapy.core.scraper] DEBUG: Scraped from <200 https://www.berlin.de/rbmskzl/>\n",
      "{'title': None}\n",
      "2025-06-05 16:58:21 [scrapy.core.scraper] DEBUG: Scraped from <200 https://www.berlin.de/rbmskzl/>\n",
      "{'title': 'Pressemitteilungen'}\n",
      "2025-06-05 16:58:21 [scrapy.core.scraper] DEBUG: Scraped from <200 https://www.berlin.de/rbmskzl/>\n",
      "{'title': None}\n",
      "2025-06-05 16:58:21 [scrapy.core.scraper] DEBUG: Scraped from <200 https://www.berlin.de/rbmskzl/>\n",
      "{'title': None}\n",
      "2025-06-05 16:58:21 [scrapy.core.scraper] DEBUG: Scraped from <200 https://www.berlin.de/rbmskzl/>\n",
      "{'title': None}\n",
      "2025-06-05 16:58:21 [scrapy.core.scraper] DEBUG: Scraped from <200 https://www.berlin.de/rbmskzl/>\n",
      "{'title': None}\n",
      "2025-06-05 16:58:21 [scrapy.core.scraper] DEBUG: Scraped from <200 https://www.berlin.de/rbmskzl/>\n",
      "{'title': None}\n",
      "2025-06-05 16:58:21 [scrapy.core.scraper] DEBUG: Scraped from <200 https://www.berlin.de/rbmskzl/>\n",
      "{'title': None}\n",
      "2025-06-05 16:58:21 [scrapy.core.scraper] DEBUG: Scraped from <200 https://www.berlin.de/rbmskzl/>\n",
      "{'title': None}\n",
      "2025-06-05 16:58:21 [scrapy.core.scraper] DEBUG: Scraped from <200 https://www.berlin.de/rbmskzl/>\n",
      "{'title': None}\n",
      "2025-06-05 16:58:21 [scrapy.core.scraper] DEBUG: Scraped from <200 https://www.berlin.de/rbmskzl/>\n",
      "{'title': None}\n",
      "2025-06-05 16:58:21 [scrapy.core.scraper] DEBUG: Scraped from <200 https://www.berlin.de/rbmskzl/>\n",
      "{'title': None}\n",
      "2025-06-05 16:58:21 [scrapy.core.scraper] DEBUG: Scraped from <200 https://www.berlin.de/rbmskzl/>\n",
      "{'title': None}\n",
      "2025-06-05 16:58:21 [scrapy.core.scraper] DEBUG: Scraped from <200 https://www.berlin.de/rbmskzl/>\n",
      "{'title': None}\n",
      "2025-06-05 16:58:21 [scrapy.core.scraper] DEBUG: Scraped from <200 https://www.berlin.de/rbmskzl/>\n",
      "{'title': None}\n",
      "2025-06-05 16:58:21 [scrapy.core.engine] INFO: Closing spider (finished)\n",
      "2025-06-05 16:58:21 [scrapy.extensions.feedexport] INFO: Stored json feed (82 items) in: output.json\n",
      "2025-06-05 16:58:21 [scrapy.statscollectors] INFO: Dumping Scrapy stats:\n",
      "{'downloader/request_bytes': 243,\n",
      " 'downloader/request_count': 1,\n",
      " 'downloader/request_method_count/GET': 1,\n",
      " 'downloader/response_bytes': 11287,\n",
      " 'downloader/response_count': 1,\n",
      " 'downloader/response_status_count/200': 1,\n",
      " 'elapsed_time_seconds': 0.283315,\n",
      " 'feedexport/success_count/FileFeedStorage': 1,\n",
      " 'finish_reason': 'finished',\n",
      " 'finish_time': datetime.datetime(2025, 6, 5, 14, 58, 21, 952865, tzinfo=datetime.timezone.utc),\n",
      " 'httpcompression/response_bytes': 52906,\n",
      " 'httpcompression/response_count': 1,\n",
      " 'item_scraped_count': 82,\n",
      " 'items_per_minute': None,\n",
      " 'log_count/DEBUG': 85,\n",
      " 'log_count/INFO': 11,\n",
      " 'memusage/max': 165330944,\n",
      " 'memusage/startup': 165330944,\n",
      " 'response_received_count': 1,\n",
      " 'responses_per_minute': None,\n",
      " 'scheduler/dequeued': 1,\n",
      " 'scheduler/dequeued/memory': 1,\n",
      " 'scheduler/enqueued': 1,\n",
      " 'scheduler/enqueued/memory': 1,\n",
      " 'start_time': datetime.datetime(2025, 6, 5, 14, 58, 21, 669550, tzinfo=datetime.timezone.utc)}\n",
      "2025-06-05 16:58:21 [scrapy.core.engine] INFO: Spider closed (finished)\n"
     ]
    }
   ],
   "source": [
    "import scrapy\n",
    "from scrapy.crawler import CrawlerProcess\n",
    "import json\n",
    "\n",
    "class SenatSpider(scrapy.Spider):\n",
    "    name = 'senat'\n",
    "    start_urls = ['https://www.berlin.de/rbmskzl/']\n",
    "\n",
    "    def parse(self, response):\n",
    "        # Extrahiere Daten von der aktuellen Seite\n",
    "        for book in response.css('div'):\n",
    "            yield {\n",
    "                'title': book.css('h2.title::text').get(),\n",
    "            }\n",
    "\n",
    "        # Folge den Links zu den n√§chsten Seiten\n",
    "        next_page = response.css('a.next-page::attr(href)').get()\n",
    "        if next_page is not None:\n",
    "            yield response.follow(next_page, self.parse)\n",
    "\n",
    "# Run the spider in Jupyter\n",
    "process = CrawlerProcess({\n",
    "    'USER_AGENT': 'mozilla/4.0 (compatible; msie 7.0; windows nt 5.1; gtb5)',\n",
    "    'FEEDS': {\n",
    "        'output.json': {'format': 'json'},\n",
    "    }\n",
    "})\n",
    "\n",
    "process.crawl(SenatSpider)\n",
    "process.start()  # This will block until crawling is finished"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb6bbe18-43f6-4e44-9556-eda3032ac8d2",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "**Vorteile:**\n",
    "- Effizientes Crawlen mehrerer Seiten\n",
    "- Integrierte Funktionen f√ºr Datenverwaltung und -export\n",
    "- Robuste Error-Handling-Mechanismen\n",
    "\n",
    "**Nachteile:**\n",
    "- Steilere Lernkurve als bei `requests`\n",
    "- Nicht geeignet f√ºr dynamische Webseiten mit JavaScript\n",
    "- Komplexere Konfiguration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2afc668b-ccb9-4079-a356-27af47458568",
   "metadata": {},
   "source": [
    "### 3. Simulation von Benutzerinteraktionen mit Selenium\n",
    "\n",
    "F√ºr Websites, die dynamische Inhalte mittels JavaScript laden oder Benutzerinteraktionen erfordern, ist `Selenium` die geeignete Wahl. Diese Bibliothek steuert einen echten Webbrowser und kann somit mit allen Elementen interagieren."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48d6cd6e-f65f-452c-8d6b-82872e26717b",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "skip-execution",
     "hide-output"
    ]
   },
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "import time\n",
    "\n",
    "# Browser-Instanz erstellen\n",
    "driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()))\n",
    "\n",
    "# Website aufrufen\n",
    "driver.get('https://example.com/dynamic-page')\n",
    "\n",
    "# Warten, bis JavaScript-Inhalte geladen sind\n",
    "time.sleep(2)\n",
    "\n",
    "# Mit Elementen interagieren\n",
    "search_button = driver.find_element(By.ID, 'search-button')\n",
    "search_button.click()\n",
    "\n",
    "# Auf dynamisch geladene Inhalte zugreifen\n",
    "results = driver.find_elements(By.CLASS_NAME, 'result-item')\n",
    "for result in results:\n",
    "    print(result.text)\n",
    "\n",
    "# Browser schlie√üen\n",
    "driver.quit()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1276daad-231e-40b4-a6fd-11ff18d50118",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "**Vorteile:**\n",
    "- Zugriff auf dynamisch geladene Inhalte (JavaScript)\n",
    "- Simulation von Benutzerinteraktionen (Klicks, Formulare ausf√ºllen)\n",
    "- \"Sieht\" die Website wie ein menschlicher Benutzer\n",
    "\n",
    "**Nachteile:**\n",
    "- Deutlich ressourcenintensiver\n",
    "- Langsamer als `requests` oder `Scrapy`\n",
    "- Anf√§lliger f√ºr √Ñnderungen im Website-Layout"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33fe211c-36e7-4f1a-9f7c-7217f0ef1a94",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## Geeignete Szenarien f√ºr die verschiedenen Methoden\n",
    "\n",
    "| Szenario | Geeignete Methode | Begr√ºndung |\n",
    "|----------|-------------------|------------|\n",
    "| Extraktion von Texten aus einer bekannten Webseite | `requests` | Einfach, effizient f√ºr einzelne statische Seiten |\n",
    "| Durchsuchen und Extraktion von Daten aus einem Blog oder Wiki | `Scrapy` | Effizientes Folgen von Links, Extrahieren √§hnlicher Daten von mehreren Seiten |\n",
    "| Daten aus einem Social-Media-Portal | `Selenium` | Notwendig f√ºr Login, Scrollen, Klicken und dynamisch nachgeladene Inhalte |\n",
    "| Korpuserstellung aus statischen Webseiten | `Scrapy` | Gute Balance aus Geschwindigkeit und Funktionalit√§t f√ºr gr√∂√üere Sammlungen |\n",
    "| Korpuserstellung aus dynamischen Webseiten | `Selenium` | Notwendig f√ºr Scrollen, Klicken und dynamisch nachgeladene Inhalte |\n",
    "| Interaktion mit Suchformularen | `Selenium` | Erm√∂glicht das Ausf√ºllen und Absenden von Formularen |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79e04fac-517e-4696-8bf8-3fad0ea00873",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## Ethische und rechtliche Aspekte\n",
    "\n",
    "Beim Web Scraping sind stets ethische und rechtliche Aspekte zu beachten:\n",
    "\n",
    "- Beachtung der `robots.txt`-Datei einer Website, die Informationen dar√ºber gibt, welche Websites gescraped werden d√ºrfen.\n",
    "- Angemessene Wartezeiten zwischen Anfragen einhalten\n",
    "- Keine pers√∂nlichen Daten ohne Einwilligung sammeln\n",
    "- Urheberrecht und Nutzungsbedingungen der Websites beachten\n",
    "- Datenschutzbestimmungen einhalten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d69293e4-5abe-4931-8fc0-d70032d68267",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
